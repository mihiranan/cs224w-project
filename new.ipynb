{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1: Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /Users/mihir/Library/Python/3.9/lib/python/site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in /Users/mihir/Library/Python/3.9/lib/python/site-packages (0.20.1)\n",
      "Requirement already satisfied: torchaudio in /Users/mihir/Library/Python/3.9/lib/python/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /Users/mihir/Library/Python/3.9/lib/python/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/mihir/Library/Python/3.9/lib/python/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/mihir/Library/Python/3.9/lib/python/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/mihir/Library/Python/3.9/lib/python/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/mihir/Library/Python/3.9/lib/python/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/mihir/Library/Python/3.9/lib/python/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/mihir/Library/Python/3.9/lib/python/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /Users/mihir/Library/Python/3.9/lib/python/site-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/mihir/Library/Python/3.9/lib/python/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mihir/Library/Python/3.9/lib/python/site-packages (from jinja2->torch) (3.0.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch-geometric==2.3.0 in /Users/mihir/Library/Python/3.9/lib/python/site-packages (2.3.0)\n",
      "Requirement already satisfied: tqdm in /Users/mihir/Library/Python/3.9/lib/python/site-packages (from torch-geometric==2.3.0) (4.67.1)\n",
      "Requirement already satisfied: numpy in /Users/mihir/Library/Python/3.9/lib/python/site-packages (from torch-geometric==2.3.0) (2.0.2)\n",
      "Requirement already satisfied: scipy in /Users/mihir/Library/Python/3.9/lib/python/site-packages (from torch-geometric==2.3.0) (1.13.1)\n",
      "Requirement already satisfied: jinja2 in /Users/mihir/Library/Python/3.9/lib/python/site-packages (from torch-geometric==2.3.0) (3.1.4)\n",
      "Requirement already satisfied: requests in /Users/mihir/Library/Python/3.9/lib/python/site-packages (from torch-geometric==2.3.0) (2.32.3)\n",
      "Requirement already satisfied: pyparsing in /Users/mihir/Library/Python/3.9/lib/python/site-packages (from torch-geometric==2.3.0) (3.2.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/mihir/Library/Python/3.9/lib/python/site-packages (from torch-geometric==2.3.0) (1.6.0)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /Users/mihir/Library/Python/3.9/lib/python/site-packages (from torch-geometric==2.3.0) (6.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mihir/Library/Python/3.9/lib/python/site-packages (from jinja2->torch-geometric==2.3.0) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mihir/Library/Python/3.9/lib/python/site-packages (from requests->torch-geometric==2.3.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mihir/Library/Python/3.9/lib/python/site-packages (from requests->torch-geometric==2.3.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mihir/Library/Python/3.9/lib/python/site-packages (from requests->torch-geometric==2.3.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mihir/Library/Python/3.9/lib/python/site-packages (from requests->torch-geometric==2.3.0) (2024.8.30)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/mihir/Library/Python/3.9/lib/python/site-packages (from scikit-learn->torch-geometric==2.3.0) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/mihir/Library/Python/3.9/lib/python/site-packages (from scikit-learn->torch-geometric==2.3.0) (3.5.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /Users/mihir/Library/Python/3.9/lib/python/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /Users/mihir/Library/Python/3.9/lib/python/site-packages (2.0.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/mihir/Library/Python/3.9/lib/python/site-packages (1.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/mihir/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mihir/Library/Python/3.9/lib/python/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/mihir/Library/Python/3.9/lib/python/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/mihir/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/mihir/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/mihir/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "zsh:1: command not found: python\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: https://data.pyg.org/whl/torch-.html\n",
      "Requirement already satisfied: torch-scatter in /Users/mihir/Library/Python/3.9/lib/python/site-packages (2.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install torch-geometric==2.3.0\n",
    "!pip install pandas numpy scikit-learn\n",
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-$(python -c \"import torch; print(torch.__version__)\").html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import degree\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_scatter import scatter_softmax\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3: Setting Device and Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# For reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4: Load and Preprocess the MovieLens Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 610\n",
      "Number of items: 9724\n",
      "Number of total nodes: 10334\n",
      "Number of interactions: 100836\n"
     ]
    }
   ],
   "source": [
    "# Ensure the dataset is in the working directory: 'ml-latest-small/ratings.csv' and 'movies.csv'\n",
    "ratings = pd.read_csv('ml-latest-small/ratings.csv')\n",
    "movies = pd.read_csv('ml-latest-small/movies.csv')\n",
    "\n",
    "# Filter out any rows with missing userIds (shouldn't happen, but just in case)\n",
    "ratings = ratings[ratings['userId'].notna()]\n",
    "\n",
    "# Map user and movie IDs to consecutive integers\n",
    "user_id_mapping = {id: idx for idx, id in enumerate(ratings['userId'].unique())}\n",
    "item_id_mapping = {id: idx for idx, id in enumerate(ratings['movieId'].unique())}\n",
    "\n",
    "ratings['userId'] = ratings['userId'].map(user_id_mapping)\n",
    "ratings['movieId'] = ratings['movieId'].map(item_id_mapping)\n",
    "\n",
    "num_users = ratings['userId'].nunique()\n",
    "num_items = ratings['movieId'].nunique()\n",
    "num_nodes = num_users + num_items\n",
    "\n",
    "print(\"Number of users:\", num_users)\n",
    "print(\"Number of items:\", num_items)\n",
    "print(\"Number of total nodes:\", num_nodes)\n",
    "print(\"Number of interactions:\", len(ratings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 5: Create Graph Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create edge index for the entire dataset\n",
    "# Users are [0, num_users-1], Items are [num_users, num_users+num_items-1]\n",
    "user_nodes = ratings['userId'].to_numpy()\n",
    "item_nodes = ratings['movieId'].to_numpy() + num_users\n",
    "\n",
    "edge_index = np.vstack((user_nodes, item_nodes))\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "\n",
    "# Edge attributes are the ratings\n",
    "edge_attr = torch.tensor(ratings['rating'].to_numpy(), dtype=torch.float32)\n",
    "\n",
    "# Feature matrix: we can start with a simple identity or zero embeddings, as LightGCN learns embeddings directly\n",
    "# We'll rely solely on the learned embeddings from the model\n",
    "data = Data(edge_index=edge_index, num_nodes=num_nodes)\n",
    "\n",
    "# Move to device\n",
    "data = data.to(device)\n",
    "edge_attr = edge_attr.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 6: Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "train_user = torch.tensor(train_data['userId'].values, dtype=torch.long, device=device)\n",
    "train_item = torch.tensor(train_data['movieId'].values + num_users, dtype=torch.long, device=device)\n",
    "train_rating = torch.tensor(train_data['rating'].values, dtype=torch.float32, device=device)\n",
    "\n",
    "test_user = torch.tensor(test_data['userId'].values, dtype=torch.long, device=device)\n",
    "test_item = torch.tensor(test_data['movieId'].values + num_users, dtype=torch.long, device=device)\n",
    "test_rating = torch.tensor(test_data['rating'].values, dtype=torch.float32, device=device)\n",
    "\n",
    "# For Recall@K calculation, we will need the test edges separately\n",
    "test_edge_index = torch.stack([test_user, test_item], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 7: Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(true_ratings, pred_ratings):\n",
    "    return np.sqrt(mean_squared_error(true_ratings, pred_ratings))\n",
    "\n",
    "def recall_at_k(model, k=10):\n",
    "    \"\"\"\n",
    "    Compute Recall@K on the test set:\n",
    "    - We consider all items and see if the items the user actually interacted with (in test set)\n",
    "      appear in the top K recommendations for that user.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get embeddings\n",
    "        embeddings = model.get_embedding(data.edge_index)\n",
    "        user_emb = embeddings[:num_users]\n",
    "        item_emb = embeddings[num_users:num_users+num_items]\n",
    "\n",
    "        # Compute scores [num_users x num_items]\n",
    "        scores = user_emb @ item_emb.T\n",
    "\n",
    "        # Get top-k items for each user\n",
    "        _, top_k_items = torch.topk(scores, k, dim=1)\n",
    "\n",
    "        # Convert test set into a dict: user -> set of test items\n",
    "        test_user_items = {}\n",
    "        for u, i, r in zip(test_data['userId'], test_data['movieId'], test_data['rating']):\n",
    "            # Only consider items where user interacted positively (rating > 0)\n",
    "            # In MovieLens all ratings > 0 by definition, but we keep the check for generality\n",
    "            if u not in test_user_items:\n",
    "                test_user_items[u] = set()\n",
    "            test_user_items[u].add(i)\n",
    "\n",
    "        recalls = []\n",
    "        for u in range(num_users):\n",
    "            if u in test_user_items and len(test_user_items[u]) > 0:\n",
    "                recommended = set((top_k_items[u].cpu().numpy()))\n",
    "                relevant = test_user_items[u]\n",
    "                hit_count = len(recommended & relevant)\n",
    "                recall_u = hit_count / len(relevant)\n",
    "                recalls.append(recall_u)\n",
    "            else:\n",
    "                # If a user has no test items, skip them or consider recall as 0\n",
    "                # Usually, we consider only users with test interactions\n",
    "                pass\n",
    "\n",
    "        if len(recalls) == 0:\n",
    "            return 0.0\n",
    "        return float(np.mean(recalls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 8: LightGCN Model (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGCNConv(MessagePassing):\n",
    "    def __init__(self):\n",
    "        super().__init__(aggr='add')\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        row, col = edge_index\n",
    "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
    "        deg_inv_sqrt = (deg + 1e-7).pow(-0.5)  # Add epsilon\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "class LightGCN(nn.Module):\n",
    "    def __init__(self, num_nodes, embedding_dim=64, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(num_nodes, embedding_dim)\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "\n",
    "        self.convs = nn.ModuleList([LightGCNConv() for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, edge_index):\n",
    "        x = self.embedding.weight\n",
    "        all_embeddings = [x]\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            all_embeddings.append(x)\n",
    "        # Mean of all layer embeddings\n",
    "        x = torch.mean(torch.stack(all_embeddings, dim=0), dim=0)\n",
    "        return x\n",
    "\n",
    "    def get_embedding(self, edge_index):\n",
    "        return self.forward(edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 9: LightGCN with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LightGCNConvWithAttention(MessagePassing):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__(aggr='add')\n",
    "        self.att = nn.Parameter(torch.Tensor(1, in_channels * 2))\n",
    "        nn.init.xavier_uniform_(self.att)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        return self.propagate(edge_index, x=x)\n",
    "\n",
    "    def message(self, x_i, x_j):\n",
    "        x_cat = torch.cat([x_i, x_j], dim=-1)\n",
    "        alpha = F.leaky_relu((x_cat * self.att).sum(dim=-1))\n",
    "        alpha = F.softmax(alpha, dim=0)\n",
    "        return alpha.unsqueeze(-1) * x_j\n",
    "\n",
    "\n",
    "\n",
    "class LightGCNWithAttention(nn.Module):\n",
    "    def __init__(self, num_nodes, embedding_dim=256, num_layers=4, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(num_nodes, embedding_dim)\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "            LightGCNConvWithAttention(embedding_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, edge_index):\n",
    "        x = self.embedding.weight\n",
    "        all_embeddings = [x]\n",
    "\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = self.layer_norm(x)\n",
    "            x = self.dropout(x)\n",
    "            all_embeddings.append(x)\n",
    "\n",
    "        return torch.stack(all_embeddings, dim=0).mean(dim=0)\n",
    "\n",
    "    def get_embedding(self, edge_index):\n",
    "        embeddings = self.forward(edge_index)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 10: Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpr_loss(user_emb, pos_item_emb, neg_item_emb):\n",
    "    pos_scores = (user_emb * pos_item_emb).sum(dim=1)\n",
    "    neg_scores = (user_emb * neg_item_emb).sum(dim=1)\n",
    "    loss = -torch.log(torch.sigmoid(pos_scores - neg_scores)).mean()\n",
    "    return loss\n",
    "\n",
    "def hybrid_loss(user_emb, pos_item_emb, neg_item_emb, pred_ratings, true_ratings, alpha=0.3):\n",
    "    # RMSE Loss\n",
    "    rmse_loss = F.mse_loss(pred_ratings, true_ratings)\n",
    "    \n",
    "    # BPR Loss\n",
    "    pos_scores = (user_emb * pos_item_emb).sum(dim=1)\n",
    "    neg_scores = (user_emb * neg_item_emb).sum(dim=1)\n",
    "    bpr_loss = -torch.log(torch.sigmoid(pos_scores - neg_scores)).mean()\n",
    "    \n",
    "    # Weighted combination\n",
    "    return alpha * rmse_loss + (1 - alpha) * bpr_loss\n",
    "\n",
    "\n",
    "\n",
    "def train_one_epoch(model, optimizer, edge_index, user, item, rating, num_items, alpha=0.3):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Get embeddings\n",
    "    embeddings = model.get_embedding(edge_index)\n",
    "    user_emb = embeddings[user]\n",
    "    pos_item_emb = embeddings[item]\n",
    "\n",
    "    # Negative sampling: randomly select negative items\n",
    "    neg_items = torch.randint(0, num_items, (len(user),), device=device)\n",
    "    neg_item_emb = embeddings[neg_items + num_users]\n",
    "\n",
    "    # Predict ratings for positive samples\n",
    "    pred_ratings = (user_emb * pos_item_emb).sum(dim=1)\n",
    "\n",
    "    # Calculate hybrid loss\n",
    "    loss = hybrid_loss(user_emb, pos_item_emb, neg_item_emb, pred_ratings, rating, alpha)\n",
    "\n",
    "    # Add L2 regularization (on embeddings)\n",
    "    l2_reg = 1e-4 * torch.norm(model.embedding.weight)\n",
    "    loss += l2_reg\n",
    "\n",
    "    # Add L2 regularization for attention (only for LightGCNWithAttention)\n",
    "    if hasattr(model, 'convs') and isinstance(model.convs[0], LightGCNConvWithAttention):\n",
    "        for conv in model.convs:\n",
    "            if hasattr(conv, 'att'):  # Check if 'att' exists\n",
    "                loss += 1e-5 * torch.norm(conv.att)\n",
    "\n",
    "    # Backpropagation and optimizer step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_rmse(model, edge_index, user, item, rating):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.get_embedding(edge_index)\n",
    "        user_emb = embeddings[user]\n",
    "        item_emb = embeddings[item]\n",
    "        pred = (user_emb * item_emb).sum(dim=1)\n",
    "        pred = torch.clamp(pred, min=0.0, max=5.0)\n",
    "        true = rating.cpu().numpy()\n",
    "        return rmse(true, pred.cpu().numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 11: Training Loops and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Baseline LightGCN...\n",
      "Epoch 001: Loss=3.4264, Train_RMSE=3.0103, Test_RMSE=3.0078\n",
      "Epoch 002: Loss=3.0872, Train_RMSE=2.7662, Test_RMSE=2.7663\n",
      "Epoch 003: Loss=2.6105, Train_RMSE=2.4396, Test_RMSE=2.4416\n",
      "Epoch 004: Loss=2.0416, Train_RMSE=2.0548, Test_RMSE=2.0574\n",
      "Epoch 005: Loss=1.4742, Train_RMSE=1.6924, Test_RMSE=1.6931\n",
      "Epoch 006: Loss=1.0560, Train_RMSE=1.4603, Test_RMSE=1.4579\n",
      "Epoch 007: Loss=0.9383, Train_RMSE=1.3689, Test_RMSE=1.3646\n",
      "Epoch 008: Loss=1.0678, Train_RMSE=1.3368, Test_RMSE=1.3330\n",
      "Epoch 009: Loss=1.1564, Train_RMSE=1.3049, Test_RMSE=1.3040\n",
      "Epoch 010: Loss=1.0724, Train_RMSE=1.2546, Test_RMSE=1.2583\n",
      "Epoch 011: Loss=0.8847, Train_RMSE=1.1919, Test_RMSE=1.2010\n",
      "Epoch 012: Loss=0.6983, Train_RMSE=1.1444, Test_RMSE=1.1610\n",
      "Epoch 013: Loss=0.5802, Train_RMSE=1.1432, Test_RMSE=1.1657\n",
      "Epoch 014: Loss=0.5444, Train_RMSE=1.1894, Test_RMSE=1.2144\n",
      "Epoch 015: Loss=0.5676, Train_RMSE=1.2526, Test_RMSE=1.2783\n",
      "Epoch 016: Loss=0.6153, Train_RMSE=1.3033, Test_RMSE=1.3293\n",
      "Epoch 017: Loss=0.6552, Train_RMSE=1.3270, Test_RMSE=1.3533\n",
      "Epoch 018: Loss=0.6741, Train_RMSE=1.3214, Test_RMSE=1.3485\n",
      "Early stopping triggered.\n",
      "Baseline LightGCN - Test RMSE: 1.3485, Recall@10: 0.0868\n",
      "\n",
      "Training LightGCN with Attention...\n",
      "Epoch 001: Loss=3.2265, Train_RMSE=3.6539, Test_RMSE=3.6513\n",
      "Epoch 002: Loss=3.2264, Train_RMSE=3.6539, Test_RMSE=3.6513\n",
      "Epoch 003: Loss=3.2262, Train_RMSE=3.6538, Test_RMSE=3.6512\n",
      "Epoch 004: Loss=3.2260, Train_RMSE=3.6536, Test_RMSE=3.6510\n",
      "Epoch 005: Loss=3.2257, Train_RMSE=3.6534, Test_RMSE=3.6508\n",
      "Epoch 006: Loss=3.2254, Train_RMSE=3.6532, Test_RMSE=3.6506\n",
      "Epoch 007: Loss=3.2249, Train_RMSE=3.6529, Test_RMSE=3.6503\n",
      "Epoch 008: Loss=3.2245, Train_RMSE=3.6525, Test_RMSE=3.6499\n",
      "Epoch 009: Loss=3.2238, Train_RMSE=3.6521, Test_RMSE=3.6495\n",
      "Epoch 010: Loss=3.2232, Train_RMSE=3.6516, Test_RMSE=3.6490\n",
      "Epoch 011: Loss=3.2224, Train_RMSE=3.6510, Test_RMSE=3.6484\n",
      "Epoch 012: Loss=3.2215, Train_RMSE=3.6504, Test_RMSE=3.6478\n",
      "Epoch 013: Loss=3.2204, Train_RMSE=3.6497, Test_RMSE=3.6471\n",
      "Epoch 014: Loss=3.2193, Train_RMSE=3.6489, Test_RMSE=3.6463\n",
      "Epoch 015: Loss=3.2181, Train_RMSE=3.6480, Test_RMSE=3.6454\n",
      "Epoch 016: Loss=3.2167, Train_RMSE=3.6470, Test_RMSE=3.6444\n",
      "Epoch 017: Loss=3.2151, Train_RMSE=3.6460, Test_RMSE=3.6434\n",
      "Epoch 018: Loss=3.2134, Train_RMSE=3.6448, Test_RMSE=3.6422\n",
      "Epoch 019: Loss=3.2118, Train_RMSE=3.6436, Test_RMSE=3.6410\n",
      "Epoch 020: Loss=3.2098, Train_RMSE=3.6422, Test_RMSE=3.6397\n",
      "Epoch 021: Loss=3.2077, Train_RMSE=3.6408, Test_RMSE=3.6382\n",
      "Epoch 022: Loss=3.2055, Train_RMSE=3.6392, Test_RMSE=3.6367\n",
      "Epoch 023: Loss=3.2030, Train_RMSE=3.6375, Test_RMSE=3.6350\n",
      "Epoch 024: Loss=3.2003, Train_RMSE=3.6358, Test_RMSE=3.6332\n",
      "Epoch 025: Loss=3.1976, Train_RMSE=3.6339, Test_RMSE=3.6314\n",
      "Epoch 026: Loss=3.1947, Train_RMSE=3.6319, Test_RMSE=3.6294\n",
      "Epoch 027: Loss=3.1918, Train_RMSE=3.6297, Test_RMSE=3.6272\n",
      "Epoch 028: Loss=3.1883, Train_RMSE=3.6275, Test_RMSE=3.6250\n",
      "Epoch 029: Loss=3.1848, Train_RMSE=3.6251, Test_RMSE=3.6226\n",
      "Epoch 030: Loss=3.1814, Train_RMSE=3.6226, Test_RMSE=3.6202\n",
      "Epoch 031: Loss=3.1774, Train_RMSE=3.6200, Test_RMSE=3.6175\n",
      "Epoch 032: Loss=3.1734, Train_RMSE=3.6172, Test_RMSE=3.6148\n",
      "Epoch 033: Loss=3.1693, Train_RMSE=3.6143, Test_RMSE=3.6119\n",
      "Epoch 034: Loss=3.1647, Train_RMSE=3.6113, Test_RMSE=3.6089\n",
      "Epoch 035: Loss=3.1604, Train_RMSE=3.6081, Test_RMSE=3.6057\n",
      "Epoch 036: Loss=3.1554, Train_RMSE=3.6048, Test_RMSE=3.6024\n",
      "Epoch 037: Loss=3.1507, Train_RMSE=3.6013, Test_RMSE=3.5990\n",
      "Epoch 038: Loss=3.1454, Train_RMSE=3.5977, Test_RMSE=3.5954\n",
      "Epoch 039: Loss=3.1396, Train_RMSE=3.5940, Test_RMSE=3.5917\n",
      "Epoch 040: Loss=3.1338, Train_RMSE=3.5901, Test_RMSE=3.5879\n",
      "Epoch 041: Loss=3.1273, Train_RMSE=3.5861, Test_RMSE=3.5838\n",
      "Epoch 042: Loss=3.1217, Train_RMSE=3.5819, Test_RMSE=3.5797\n",
      "Epoch 043: Loss=3.1153, Train_RMSE=3.5775, Test_RMSE=3.5753\n",
      "Epoch 044: Loss=3.1087, Train_RMSE=3.5730, Test_RMSE=3.5709\n",
      "Epoch 045: Loss=3.1020, Train_RMSE=3.5684, Test_RMSE=3.5662\n",
      "Epoch 046: Loss=3.0948, Train_RMSE=3.5635, Test_RMSE=3.5615\n",
      "Epoch 047: Loss=3.0881, Train_RMSE=3.5586, Test_RMSE=3.5565\n",
      "Epoch 048: Loss=3.0801, Train_RMSE=3.5534, Test_RMSE=3.5514\n",
      "Epoch 049: Loss=3.0728, Train_RMSE=3.5481, Test_RMSE=3.5461\n",
      "Epoch 050: Loss=3.0641, Train_RMSE=3.5427, Test_RMSE=3.5407\n",
      "Epoch 051: Loss=3.0567, Train_RMSE=3.5371, Test_RMSE=3.5351\n",
      "Epoch 052: Loss=3.0483, Train_RMSE=3.5313, Test_RMSE=3.5294\n",
      "Epoch 053: Loss=3.0400, Train_RMSE=3.5253, Test_RMSE=3.5235\n",
      "Epoch 054: Loss=3.0306, Train_RMSE=3.5192, Test_RMSE=3.5174\n",
      "Epoch 055: Loss=3.0208, Train_RMSE=3.5129, Test_RMSE=3.5111\n",
      "Epoch 056: Loss=3.0123, Train_RMSE=3.5065, Test_RMSE=3.5047\n",
      "Epoch 057: Loss=3.0019, Train_RMSE=3.4998, Test_RMSE=3.4981\n",
      "Epoch 058: Loss=2.9916, Train_RMSE=3.4930, Test_RMSE=3.4914\n",
      "Epoch 059: Loss=2.9831, Train_RMSE=3.4861, Test_RMSE=3.4845\n",
      "Epoch 060: Loss=2.9704, Train_RMSE=3.4789, Test_RMSE=3.4774\n",
      "Epoch 061: Loss=2.9615, Train_RMSE=3.4716, Test_RMSE=3.4701\n",
      "Epoch 062: Loss=2.9500, Train_RMSE=3.4641, Test_RMSE=3.4627\n",
      "Epoch 063: Loss=2.9404, Train_RMSE=3.4565, Test_RMSE=3.4551\n",
      "Epoch 064: Loss=2.9282, Train_RMSE=3.4486, Test_RMSE=3.4473\n",
      "Epoch 065: Loss=2.9167, Train_RMSE=3.4406, Test_RMSE=3.4393\n",
      "Epoch 066: Loss=2.9057, Train_RMSE=3.4324, Test_RMSE=3.4312\n",
      "Epoch 067: Loss=2.8936, Train_RMSE=3.4241, Test_RMSE=3.4229\n",
      "Epoch 068: Loss=2.8792, Train_RMSE=3.4155, Test_RMSE=3.4144\n",
      "Epoch 069: Loss=2.8668, Train_RMSE=3.4068, Test_RMSE=3.4058\n",
      "Epoch 070: Loss=2.8555, Train_RMSE=3.3979, Test_RMSE=3.3969\n",
      "Epoch 071: Loss=2.8424, Train_RMSE=3.3889, Test_RMSE=3.3879\n",
      "Epoch 072: Loss=2.8295, Train_RMSE=3.3796, Test_RMSE=3.3788\n",
      "Epoch 073: Loss=2.8155, Train_RMSE=3.3702, Test_RMSE=3.3694\n",
      "Epoch 074: Loss=2.8028, Train_RMSE=3.3606, Test_RMSE=3.3599\n",
      "Epoch 075: Loss=2.7901, Train_RMSE=3.3509, Test_RMSE=3.3502\n",
      "Epoch 076: Loss=2.7749, Train_RMSE=3.3409, Test_RMSE=3.3403\n",
      "Epoch 077: Loss=2.7588, Train_RMSE=3.3308, Test_RMSE=3.3302\n",
      "Epoch 078: Loss=2.7474, Train_RMSE=3.3205, Test_RMSE=3.3200\n",
      "Epoch 079: Loss=2.7318, Train_RMSE=3.3100, Test_RMSE=3.3096\n",
      "Epoch 080: Loss=2.7180, Train_RMSE=3.2994, Test_RMSE=3.2990\n",
      "Epoch 081: Loss=2.7009, Train_RMSE=3.2885, Test_RMSE=3.2883\n",
      "Epoch 082: Loss=2.6852, Train_RMSE=3.2775, Test_RMSE=3.2774\n",
      "Epoch 083: Loss=2.6708, Train_RMSE=3.2663, Test_RMSE=3.2663\n",
      "Epoch 084: Loss=2.6541, Train_RMSE=3.2550, Test_RMSE=3.2550\n",
      "Epoch 085: Loss=2.6393, Train_RMSE=3.2434, Test_RMSE=3.2435\n",
      "Epoch 086: Loss=2.6246, Train_RMSE=3.2317, Test_RMSE=3.2319\n",
      "Epoch 087: Loss=2.6065, Train_RMSE=3.2198, Test_RMSE=3.2201\n",
      "Epoch 088: Loss=2.5884, Train_RMSE=3.2077, Test_RMSE=3.2081\n",
      "Epoch 089: Loss=2.5750, Train_RMSE=3.1955, Test_RMSE=3.1960\n",
      "Epoch 090: Loss=2.5561, Train_RMSE=3.1831, Test_RMSE=3.1836\n",
      "Epoch 091: Loss=2.5432, Train_RMSE=3.1705, Test_RMSE=3.1711\n",
      "Epoch 092: Loss=2.5236, Train_RMSE=3.1577, Test_RMSE=3.1585\n",
      "Epoch 093: Loss=2.5067, Train_RMSE=3.1448, Test_RMSE=3.1456\n",
      "Epoch 094: Loss=2.4861, Train_RMSE=3.1316, Test_RMSE=3.1326\n",
      "Epoch 095: Loss=2.4748, Train_RMSE=3.1183, Test_RMSE=3.1194\n",
      "Epoch 096: Loss=2.4533, Train_RMSE=3.1049, Test_RMSE=3.1060\n",
      "Epoch 097: Loss=2.4346, Train_RMSE=3.0912, Test_RMSE=3.0925\n",
      "Epoch 098: Loss=2.4223, Train_RMSE=3.0774, Test_RMSE=3.0788\n",
      "Epoch 099: Loss=2.4006, Train_RMSE=3.0634, Test_RMSE=3.0649\n",
      "Epoch 100: Loss=2.3753, Train_RMSE=3.0492, Test_RMSE=3.0509\n",
      "Epoch 101: Loss=2.3620, Train_RMSE=3.0349, Test_RMSE=3.0366\n",
      "Epoch 102: Loss=2.3442, Train_RMSE=3.0204, Test_RMSE=3.0222\n",
      "Epoch 103: Loss=2.3255, Train_RMSE=3.0056, Test_RMSE=3.0077\n",
      "Epoch 104: Loss=2.3049, Train_RMSE=2.9908, Test_RMSE=2.9929\n",
      "Epoch 105: Loss=2.2835, Train_RMSE=2.9757, Test_RMSE=2.9780\n",
      "Epoch 106: Loss=2.2670, Train_RMSE=2.9604, Test_RMSE=2.9629\n",
      "Epoch 107: Loss=2.2486, Train_RMSE=2.9450, Test_RMSE=2.9476\n",
      "Epoch 108: Loss=2.2280, Train_RMSE=2.9294, Test_RMSE=2.9321\n",
      "Epoch 109: Loss=2.2121, Train_RMSE=2.9136, Test_RMSE=2.9165\n",
      "Epoch 110: Loss=2.1893, Train_RMSE=2.8976, Test_RMSE=2.9007\n",
      "Epoch 111: Loss=2.1723, Train_RMSE=2.8815, Test_RMSE=2.8847\n",
      "Epoch 112: Loss=2.1500, Train_RMSE=2.8652, Test_RMSE=2.8686\n",
      "Epoch 113: Loss=2.1234, Train_RMSE=2.8487, Test_RMSE=2.8523\n",
      "Epoch 114: Loss=2.1084, Train_RMSE=2.8320, Test_RMSE=2.8358\n",
      "Epoch 115: Loss=2.0828, Train_RMSE=2.8152, Test_RMSE=2.8192\n",
      "Epoch 116: Loss=2.0652, Train_RMSE=2.7982, Test_RMSE=2.8024\n",
      "Epoch 117: Loss=2.0468, Train_RMSE=2.7811, Test_RMSE=2.7855\n",
      "Epoch 118: Loss=2.0264, Train_RMSE=2.7639, Test_RMSE=2.7685\n",
      "Epoch 119: Loss=2.0022, Train_RMSE=2.7466, Test_RMSE=2.7514\n",
      "Epoch 120: Loss=1.9810, Train_RMSE=2.7293, Test_RMSE=2.7342\n",
      "Epoch 121: Loss=1.9634, Train_RMSE=2.7119, Test_RMSE=2.7171\n",
      "Epoch 122: Loss=1.9428, Train_RMSE=2.6946, Test_RMSE=2.6999\n",
      "Epoch 123: Loss=1.9260, Train_RMSE=2.6774, Test_RMSE=2.6828\n",
      "Epoch 124: Loss=1.9080, Train_RMSE=2.6603, Test_RMSE=2.6658\n",
      "Epoch 125: Loss=1.8820, Train_RMSE=2.6432, Test_RMSE=2.6488\n",
      "Epoch 126: Loss=1.8659, Train_RMSE=2.6261, Test_RMSE=2.6318\n",
      "Epoch 127: Loss=1.8509, Train_RMSE=2.6090, Test_RMSE=2.6148\n",
      "Epoch 128: Loss=1.8356, Train_RMSE=2.5920, Test_RMSE=2.5978\n",
      "Epoch 129: Loss=1.8150, Train_RMSE=2.5749, Test_RMSE=2.5807\n",
      "Epoch 130: Loss=1.7923, Train_RMSE=2.5578, Test_RMSE=2.5636\n",
      "Epoch 131: Loss=1.7712, Train_RMSE=2.5406, Test_RMSE=2.5465\n",
      "Epoch 132: Loss=1.7597, Train_RMSE=2.5233, Test_RMSE=2.5293\n",
      "Epoch 133: Loss=1.7240, Train_RMSE=2.5061, Test_RMSE=2.5121\n",
      "Epoch 134: Loss=1.7173, Train_RMSE=2.4888, Test_RMSE=2.4949\n",
      "Epoch 135: Loss=1.6944, Train_RMSE=2.4715, Test_RMSE=2.4777\n",
      "Epoch 136: Loss=1.6778, Train_RMSE=2.4543, Test_RMSE=2.4605\n",
      "Epoch 137: Loss=1.6573, Train_RMSE=2.4370, Test_RMSE=2.4433\n",
      "Epoch 138: Loss=1.6586, Train_RMSE=2.4198, Test_RMSE=2.4262\n",
      "Epoch 139: Loss=1.6321, Train_RMSE=2.4026, Test_RMSE=2.4091\n",
      "Epoch 140: Loss=1.6097, Train_RMSE=2.3854, Test_RMSE=2.3921\n",
      "Epoch 141: Loss=1.5838, Train_RMSE=2.3683, Test_RMSE=2.3750\n",
      "Epoch 142: Loss=1.5729, Train_RMSE=2.3511, Test_RMSE=2.3580\n",
      "Epoch 143: Loss=1.5497, Train_RMSE=2.3340, Test_RMSE=2.3410\n",
      "Epoch 144: Loss=1.5420, Train_RMSE=2.3170, Test_RMSE=2.3241\n",
      "Epoch 145: Loss=1.5206, Train_RMSE=2.3000, Test_RMSE=2.3072\n",
      "Epoch 146: Loss=1.5117, Train_RMSE=2.2830, Test_RMSE=2.2904\n",
      "Epoch 147: Loss=1.4914, Train_RMSE=2.2661, Test_RMSE=2.2737\n",
      "Epoch 148: Loss=1.4747, Train_RMSE=2.2493, Test_RMSE=2.2570\n",
      "Epoch 149: Loss=1.4591, Train_RMSE=2.2326, Test_RMSE=2.2404\n",
      "Epoch 150: Loss=1.4393, Train_RMSE=2.2159, Test_RMSE=2.2238\n",
      "Epoch 151: Loss=1.4261, Train_RMSE=2.1993, Test_RMSE=2.2074\n",
      "Epoch 152: Loss=1.4038, Train_RMSE=2.1828, Test_RMSE=2.1910\n",
      "Epoch 153: Loss=1.3937, Train_RMSE=2.1664, Test_RMSE=2.1747\n",
      "Epoch 154: Loss=1.3755, Train_RMSE=2.1502, Test_RMSE=2.1586\n",
      "Epoch 155: Loss=1.3741, Train_RMSE=2.1340, Test_RMSE=2.1426\n",
      "Epoch 156: Loss=1.3467, Train_RMSE=2.1179, Test_RMSE=2.1266\n",
      "Epoch 157: Loss=1.3288, Train_RMSE=2.1019, Test_RMSE=2.1107\n",
      "Epoch 158: Loss=1.3194, Train_RMSE=2.0860, Test_RMSE=2.0950\n",
      "Epoch 159: Loss=1.3114, Train_RMSE=2.0701, Test_RMSE=2.0793\n",
      "Epoch 160: Loss=1.2973, Train_RMSE=2.0544, Test_RMSE=2.0637\n",
      "Epoch 161: Loss=1.2752, Train_RMSE=2.0388, Test_RMSE=2.0483\n",
      "Epoch 162: Loss=1.2584, Train_RMSE=2.0233, Test_RMSE=2.0329\n",
      "Epoch 163: Loss=1.2439, Train_RMSE=2.0078, Test_RMSE=2.0176\n",
      "Epoch 164: Loss=1.2366, Train_RMSE=1.9925, Test_RMSE=2.0024\n",
      "Epoch 165: Loss=1.2127, Train_RMSE=1.9773, Test_RMSE=1.9874\n",
      "Epoch 166: Loss=1.2146, Train_RMSE=1.9622, Test_RMSE=1.9724\n",
      "Epoch 167: Loss=1.1929, Train_RMSE=1.9473, Test_RMSE=1.9576\n",
      "Epoch 168: Loss=1.1763, Train_RMSE=1.9324, Test_RMSE=1.9429\n",
      "Epoch 169: Loss=1.1689, Train_RMSE=1.9176, Test_RMSE=1.9283\n",
      "Epoch 170: Loss=1.1707, Train_RMSE=1.9030, Test_RMSE=1.9139\n",
      "Epoch 171: Loss=1.1462, Train_RMSE=1.8885, Test_RMSE=1.8995\n",
      "Epoch 172: Loss=1.1318, Train_RMSE=1.8742, Test_RMSE=1.8853\n",
      "Epoch 173: Loss=1.1306, Train_RMSE=1.8599, Test_RMSE=1.8712\n",
      "Epoch 174: Loss=1.1066, Train_RMSE=1.8458, Test_RMSE=1.8573\n",
      "Epoch 175: Loss=1.1013, Train_RMSE=1.8318, Test_RMSE=1.8434\n",
      "Epoch 176: Loss=1.0882, Train_RMSE=1.8179, Test_RMSE=1.8297\n",
      "Epoch 177: Loss=1.0826, Train_RMSE=1.8042, Test_RMSE=1.8162\n",
      "Epoch 178: Loss=1.0676, Train_RMSE=1.7906, Test_RMSE=1.8027\n",
      "Epoch 179: Loss=1.0552, Train_RMSE=1.7771, Test_RMSE=1.7894\n",
      "Epoch 180: Loss=1.0521, Train_RMSE=1.7638, Test_RMSE=1.7762\n",
      "Epoch 181: Loss=1.0313, Train_RMSE=1.7506, Test_RMSE=1.7632\n",
      "Epoch 182: Loss=1.0262, Train_RMSE=1.7376, Test_RMSE=1.7503\n",
      "Epoch 183: Loss=1.0219, Train_RMSE=1.7247, Test_RMSE=1.7376\n",
      "Epoch 184: Loss=1.0025, Train_RMSE=1.7120, Test_RMSE=1.7250\n",
      "Epoch 185: Loss=0.9959, Train_RMSE=1.6994, Test_RMSE=1.7126\n",
      "Epoch 186: Loss=0.9824, Train_RMSE=1.6870, Test_RMSE=1.7003\n",
      "Epoch 187: Loss=0.9781, Train_RMSE=1.6747, Test_RMSE=1.6882\n",
      "Epoch 188: Loss=0.9703, Train_RMSE=1.6626, Test_RMSE=1.6763\n",
      "Epoch 189: Loss=0.9619, Train_RMSE=1.6507, Test_RMSE=1.6645\n",
      "Epoch 190: Loss=0.9581, Train_RMSE=1.6389, Test_RMSE=1.6529\n",
      "Epoch 191: Loss=0.9441, Train_RMSE=1.6273, Test_RMSE=1.6414\n",
      "Epoch 192: Loss=0.9356, Train_RMSE=1.6159, Test_RMSE=1.6301\n",
      "Epoch 193: Loss=0.9280, Train_RMSE=1.6046, Test_RMSE=1.6190\n",
      "Epoch 194: Loss=0.9191, Train_RMSE=1.5935, Test_RMSE=1.6080\n",
      "Epoch 195: Loss=0.9161, Train_RMSE=1.5825, Test_RMSE=1.5972\n",
      "Epoch 196: Loss=0.9060, Train_RMSE=1.5717, Test_RMSE=1.5865\n",
      "Epoch 197: Loss=0.8996, Train_RMSE=1.5611, Test_RMSE=1.5761\n",
      "Epoch 198: Loss=0.8840, Train_RMSE=1.5507, Test_RMSE=1.5657\n",
      "Epoch 199: Loss=0.8813, Train_RMSE=1.5404, Test_RMSE=1.5556\n",
      "Epoch 200: Loss=0.8758, Train_RMSE=1.5302, Test_RMSE=1.5456\n",
      "Epoch 201: Loss=0.8693, Train_RMSE=1.5203, Test_RMSE=1.5357\n",
      "Epoch 202: Loss=0.8653, Train_RMSE=1.5104, Test_RMSE=1.5260\n",
      "Epoch 203: Loss=0.8561, Train_RMSE=1.5008, Test_RMSE=1.5165\n",
      "Epoch 204: Loss=0.8512, Train_RMSE=1.4913, Test_RMSE=1.5071\n",
      "Epoch 205: Loss=0.8440, Train_RMSE=1.4820, Test_RMSE=1.4979\n",
      "Epoch 206: Loss=0.8355, Train_RMSE=1.4728, Test_RMSE=1.4889\n",
      "Epoch 207: Loss=0.8327, Train_RMSE=1.4638, Test_RMSE=1.4800\n",
      "Epoch 208: Loss=0.8286, Train_RMSE=1.4549, Test_RMSE=1.4712\n",
      "Epoch 209: Loss=0.8252, Train_RMSE=1.4462, Test_RMSE=1.4626\n",
      "Epoch 210: Loss=0.8170, Train_RMSE=1.4377, Test_RMSE=1.4542\n",
      "Epoch 211: Loss=0.8132, Train_RMSE=1.4292, Test_RMSE=1.4459\n",
      "Epoch 212: Loss=0.8050, Train_RMSE=1.4210, Test_RMSE=1.4377\n",
      "Epoch 213: Loss=0.8007, Train_RMSE=1.4129, Test_RMSE=1.4297\n",
      "Epoch 214: Loss=0.7967, Train_RMSE=1.4049, Test_RMSE=1.4219\n",
      "Epoch 215: Loss=0.7884, Train_RMSE=1.3972, Test_RMSE=1.4142\n",
      "Epoch 216: Loss=0.7887, Train_RMSE=1.3895, Test_RMSE=1.4066\n",
      "Epoch 217: Loss=0.7772, Train_RMSE=1.3820, Test_RMSE=1.3993\n",
      "Epoch 218: Loss=0.7786, Train_RMSE=1.3747, Test_RMSE=1.3920\n",
      "Epoch 219: Loss=0.7701, Train_RMSE=1.3675, Test_RMSE=1.3849\n",
      "Epoch 220: Loss=0.7711, Train_RMSE=1.3604, Test_RMSE=1.3779\n",
      "Epoch 221: Loss=0.7677, Train_RMSE=1.3535, Test_RMSE=1.3711\n",
      "Epoch 222: Loss=0.7611, Train_RMSE=1.3467, Test_RMSE=1.3644\n",
      "Epoch 223: Loss=0.7536, Train_RMSE=1.3400, Test_RMSE=1.3578\n",
      "Epoch 224: Loss=0.7572, Train_RMSE=1.3335, Test_RMSE=1.3513\n",
      "Epoch 225: Loss=0.7481, Train_RMSE=1.3271, Test_RMSE=1.3450\n",
      "Epoch 226: Loss=0.7452, Train_RMSE=1.3208, Test_RMSE=1.3388\n",
      "Epoch 227: Loss=0.7434, Train_RMSE=1.3147, Test_RMSE=1.3328\n",
      "Epoch 228: Loss=0.7378, Train_RMSE=1.3087, Test_RMSE=1.3268\n",
      "Epoch 229: Loss=0.7323, Train_RMSE=1.3029, Test_RMSE=1.3211\n",
      "Epoch 230: Loss=0.7296, Train_RMSE=1.2971, Test_RMSE=1.3154\n",
      "Epoch 231: Loss=0.7269, Train_RMSE=1.2915, Test_RMSE=1.3098\n",
      "Epoch 232: Loss=0.7209, Train_RMSE=1.2860, Test_RMSE=1.3044\n",
      "Epoch 233: Loss=0.7221, Train_RMSE=1.2806, Test_RMSE=1.2991\n",
      "Epoch 234: Loss=0.7176, Train_RMSE=1.2754, Test_RMSE=1.2939\n",
      "Epoch 235: Loss=0.7120, Train_RMSE=1.2702, Test_RMSE=1.2888\n",
      "Epoch 236: Loss=0.7107, Train_RMSE=1.2652, Test_RMSE=1.2838\n",
      "Epoch 237: Loss=0.7099, Train_RMSE=1.2603, Test_RMSE=1.2790\n",
      "Epoch 238: Loss=0.7070, Train_RMSE=1.2554, Test_RMSE=1.2742\n",
      "Epoch 239: Loss=0.7057, Train_RMSE=1.2507, Test_RMSE=1.2695\n",
      "Epoch 240: Loss=0.7048, Train_RMSE=1.2461, Test_RMSE=1.2650\n",
      "Epoch 241: Loss=0.7004, Train_RMSE=1.2415, Test_RMSE=1.2605\n",
      "Epoch 242: Loss=0.6947, Train_RMSE=1.2371, Test_RMSE=1.2561\n",
      "Epoch 243: Loss=0.6937, Train_RMSE=1.2328, Test_RMSE=1.2518\n",
      "Epoch 244: Loss=0.6900, Train_RMSE=1.2285, Test_RMSE=1.2476\n",
      "Epoch 245: Loss=0.6937, Train_RMSE=1.2244, Test_RMSE=1.2435\n",
      "Epoch 246: Loss=0.6890, Train_RMSE=1.2203, Test_RMSE=1.2395\n",
      "Epoch 247: Loss=0.6847, Train_RMSE=1.2163, Test_RMSE=1.2356\n",
      "Epoch 248: Loss=0.6822, Train_RMSE=1.2125, Test_RMSE=1.2318\n",
      "Epoch 249: Loss=0.6839, Train_RMSE=1.2087, Test_RMSE=1.2281\n",
      "Epoch 250: Loss=0.6781, Train_RMSE=1.2050, Test_RMSE=1.2244\n",
      "Epoch 251: Loss=0.6809, Train_RMSE=1.2013, Test_RMSE=1.2208\n",
      "Epoch 252: Loss=0.6779, Train_RMSE=1.1978, Test_RMSE=1.2173\n",
      "Epoch 253: Loss=0.6749, Train_RMSE=1.1943, Test_RMSE=1.2139\n",
      "Epoch 254: Loss=0.6728, Train_RMSE=1.1909, Test_RMSE=1.2105\n",
      "Epoch 255: Loss=0.6735, Train_RMSE=1.1876, Test_RMSE=1.2073\n",
      "Epoch 256: Loss=0.6707, Train_RMSE=1.1843, Test_RMSE=1.2040\n",
      "Epoch 257: Loss=0.6656, Train_RMSE=1.1811, Test_RMSE=1.2009\n",
      "Epoch 258: Loss=0.6655, Train_RMSE=1.1780, Test_RMSE=1.1978\n",
      "Epoch 259: Loss=0.6672, Train_RMSE=1.1749, Test_RMSE=1.1947\n",
      "Epoch 260: Loss=0.6631, Train_RMSE=1.1719, Test_RMSE=1.1918\n",
      "Epoch 261: Loss=0.6628, Train_RMSE=1.1690, Test_RMSE=1.1889\n",
      "Epoch 262: Loss=0.6621, Train_RMSE=1.1661, Test_RMSE=1.1860\n",
      "Epoch 263: Loss=0.6576, Train_RMSE=1.1633, Test_RMSE=1.1832\n",
      "Epoch 264: Loss=0.6588, Train_RMSE=1.1605, Test_RMSE=1.1805\n",
      "Epoch 265: Loss=0.6568, Train_RMSE=1.1578, Test_RMSE=1.1778\n",
      "Epoch 266: Loss=0.6569, Train_RMSE=1.1552, Test_RMSE=1.1752\n",
      "Epoch 267: Loss=0.6517, Train_RMSE=1.1526, Test_RMSE=1.1727\n",
      "Epoch 268: Loss=0.6524, Train_RMSE=1.1501, Test_RMSE=1.1702\n",
      "Epoch 269: Loss=0.6526, Train_RMSE=1.1476, Test_RMSE=1.1677\n",
      "Epoch 270: Loss=0.6471, Train_RMSE=1.1452, Test_RMSE=1.1653\n",
      "Epoch 271: Loss=0.6477, Train_RMSE=1.1428, Test_RMSE=1.1630\n",
      "Epoch 272: Loss=0.6471, Train_RMSE=1.1405, Test_RMSE=1.1607\n",
      "Epoch 273: Loss=0.6485, Train_RMSE=1.1383, Test_RMSE=1.1585\n",
      "Epoch 274: Loss=0.6468, Train_RMSE=1.1361, Test_RMSE=1.1563\n",
      "Epoch 275: Loss=0.6440, Train_RMSE=1.1339, Test_RMSE=1.1542\n",
      "Epoch 276: Loss=0.6423, Train_RMSE=1.1318, Test_RMSE=1.1521\n",
      "Epoch 277: Loss=0.6404, Train_RMSE=1.1297, Test_RMSE=1.1500\n",
      "Epoch 278: Loss=0.6448, Train_RMSE=1.1277, Test_RMSE=1.1480\n",
      "Epoch 279: Loss=0.6405, Train_RMSE=1.1257, Test_RMSE=1.1460\n",
      "Epoch 280: Loss=0.6411, Train_RMSE=1.1237, Test_RMSE=1.1441\n",
      "Epoch 281: Loss=0.6388, Train_RMSE=1.1218, Test_RMSE=1.1422\n",
      "Epoch 282: Loss=0.6384, Train_RMSE=1.1199, Test_RMSE=1.1403\n",
      "Epoch 283: Loss=0.6359, Train_RMSE=1.1180, Test_RMSE=1.1384\n",
      "Epoch 284: Loss=0.6396, Train_RMSE=1.1162, Test_RMSE=1.1366\n",
      "Epoch 285: Loss=0.6344, Train_RMSE=1.1144, Test_RMSE=1.1348\n",
      "Epoch 286: Loss=0.6357, Train_RMSE=1.1126, Test_RMSE=1.1331\n",
      "Epoch 287: Loss=0.6368, Train_RMSE=1.1109, Test_RMSE=1.1313\n",
      "Epoch 288: Loss=0.6307, Train_RMSE=1.1092, Test_RMSE=1.1296\n",
      "Epoch 289: Loss=0.6337, Train_RMSE=1.1075, Test_RMSE=1.1279\n",
      "Epoch 290: Loss=0.6344, Train_RMSE=1.1059, Test_RMSE=1.1263\n",
      "Epoch 291: Loss=0.6291, Train_RMSE=1.1043, Test_RMSE=1.1247\n",
      "Epoch 292: Loss=0.6267, Train_RMSE=1.1027, Test_RMSE=1.1231\n",
      "Epoch 293: Loss=0.6319, Train_RMSE=1.1012, Test_RMSE=1.1216\n",
      "Epoch 294: Loss=0.6319, Train_RMSE=1.0997, Test_RMSE=1.1201\n",
      "Epoch 295: Loss=0.6280, Train_RMSE=1.0982, Test_RMSE=1.1186\n",
      "Epoch 296: Loss=0.6296, Train_RMSE=1.0967, Test_RMSE=1.1172\n",
      "Epoch 297: Loss=0.6287, Train_RMSE=1.0953, Test_RMSE=1.1157\n",
      "Epoch 298: Loss=0.6290, Train_RMSE=1.0939, Test_RMSE=1.1143\n",
      "Epoch 299: Loss=0.6250, Train_RMSE=1.0925, Test_RMSE=1.1129\n",
      "Epoch 300: Loss=0.6250, Train_RMSE=1.0912, Test_RMSE=1.1116\n",
      "Epoch 301: Loss=0.6260, Train_RMSE=1.0899, Test_RMSE=1.1103\n",
      "Epoch 302: Loss=0.6243, Train_RMSE=1.0885, Test_RMSE=1.1089\n",
      "Epoch 303: Loss=0.6249, Train_RMSE=1.0872, Test_RMSE=1.1076\n",
      "Epoch 304: Loss=0.6252, Train_RMSE=1.0860, Test_RMSE=1.1064\n",
      "Epoch 305: Loss=0.6200, Train_RMSE=1.0847, Test_RMSE=1.1051\n",
      "Epoch 306: Loss=0.6258, Train_RMSE=1.0835, Test_RMSE=1.1038\n",
      "Epoch 307: Loss=0.6225, Train_RMSE=1.0822, Test_RMSE=1.1026\n",
      "Epoch 308: Loss=0.6224, Train_RMSE=1.0810, Test_RMSE=1.1014\n",
      "Epoch 309: Loss=0.6207, Train_RMSE=1.0799, Test_RMSE=1.1002\n",
      "Epoch 310: Loss=0.6199, Train_RMSE=1.0787, Test_RMSE=1.0990\n",
      "Epoch 311: Loss=0.6197, Train_RMSE=1.0775, Test_RMSE=1.0979\n",
      "Epoch 312: Loss=0.6199, Train_RMSE=1.0764, Test_RMSE=1.0967\n",
      "Epoch 313: Loss=0.6194, Train_RMSE=1.0753, Test_RMSE=1.0956\n",
      "Epoch 314: Loss=0.6208, Train_RMSE=1.0742, Test_RMSE=1.0945\n",
      "Epoch 315: Loss=0.6195, Train_RMSE=1.0731, Test_RMSE=1.0934\n",
      "Epoch 316: Loss=0.6186, Train_RMSE=1.0721, Test_RMSE=1.0923\n",
      "Epoch 317: Loss=0.6173, Train_RMSE=1.0710, Test_RMSE=1.0913\n",
      "Epoch 318: Loss=0.6169, Train_RMSE=1.0700, Test_RMSE=1.0902\n",
      "Epoch 319: Loss=0.6158, Train_RMSE=1.0690, Test_RMSE=1.0892\n",
      "Epoch 320: Loss=0.6170, Train_RMSE=1.0680, Test_RMSE=1.0882\n",
      "Epoch 321: Loss=0.6165, Train_RMSE=1.0670, Test_RMSE=1.0872\n",
      "Epoch 322: Loss=0.6156, Train_RMSE=1.0660, Test_RMSE=1.0862\n",
      "Epoch 323: Loss=0.6156, Train_RMSE=1.0650, Test_RMSE=1.0852\n",
      "Epoch 324: Loss=0.6141, Train_RMSE=1.0640, Test_RMSE=1.0842\n",
      "Epoch 325: Loss=0.6144, Train_RMSE=1.0631, Test_RMSE=1.0833\n",
      "Epoch 326: Loss=0.6151, Train_RMSE=1.0621, Test_RMSE=1.0824\n",
      "Epoch 327: Loss=0.6148, Train_RMSE=1.0612, Test_RMSE=1.0814\n",
      "Epoch 328: Loss=0.6151, Train_RMSE=1.0603, Test_RMSE=1.0805\n",
      "Epoch 329: Loss=0.6133, Train_RMSE=1.0594, Test_RMSE=1.0796\n",
      "Epoch 330: Loss=0.6163, Train_RMSE=1.0585, Test_RMSE=1.0787\n",
      "Epoch 331: Loss=0.6127, Train_RMSE=1.0576, Test_RMSE=1.0779\n",
      "Epoch 332: Loss=0.6131, Train_RMSE=1.0567, Test_RMSE=1.0770\n",
      "Epoch 333: Loss=0.6115, Train_RMSE=1.0558, Test_RMSE=1.0761\n",
      "Epoch 334: Loss=0.6128, Train_RMSE=1.0550, Test_RMSE=1.0753\n",
      "Epoch 335: Loss=0.6128, Train_RMSE=1.0541, Test_RMSE=1.0744\n",
      "Epoch 336: Loss=0.6098, Train_RMSE=1.0533, Test_RMSE=1.0735\n",
      "Epoch 337: Loss=0.6101, Train_RMSE=1.0524, Test_RMSE=1.0727\n",
      "Epoch 338: Loss=0.6132, Train_RMSE=1.0516, Test_RMSE=1.0719\n",
      "Epoch 339: Loss=0.6141, Train_RMSE=1.0508, Test_RMSE=1.0710\n",
      "Epoch 340: Loss=0.6096, Train_RMSE=1.0500, Test_RMSE=1.0702\n",
      "Epoch 341: Loss=0.6119, Train_RMSE=1.0491, Test_RMSE=1.0694\n",
      "Epoch 342: Loss=0.6089, Train_RMSE=1.0484, Test_RMSE=1.0686\n",
      "Epoch 343: Loss=0.6090, Train_RMSE=1.0476, Test_RMSE=1.0678\n",
      "Epoch 344: Loss=0.6094, Train_RMSE=1.0468, Test_RMSE=1.0671\n",
      "Epoch 345: Loss=0.6103, Train_RMSE=1.0460, Test_RMSE=1.0663\n",
      "Epoch 346: Loss=0.6107, Train_RMSE=1.0453, Test_RMSE=1.0655\n",
      "Epoch 347: Loss=0.6093, Train_RMSE=1.0445, Test_RMSE=1.0648\n",
      "Epoch 348: Loss=0.6085, Train_RMSE=1.0438, Test_RMSE=1.0641\n",
      "Epoch 349: Loss=0.6064, Train_RMSE=1.0430, Test_RMSE=1.0633\n",
      "Epoch 350: Loss=0.6090, Train_RMSE=1.0423, Test_RMSE=1.0626\n",
      "Epoch 351: Loss=0.6068, Train_RMSE=1.0416, Test_RMSE=1.0619\n",
      "Epoch 352: Loss=0.6057, Train_RMSE=1.0409, Test_RMSE=1.0612\n",
      "Epoch 353: Loss=0.6076, Train_RMSE=1.0402, Test_RMSE=1.0604\n",
      "Epoch 354: Loss=0.6064, Train_RMSE=1.0395, Test_RMSE=1.0597\n",
      "Epoch 355: Loss=0.6090, Train_RMSE=1.0388, Test_RMSE=1.0591\n",
      "Epoch 356: Loss=0.6075, Train_RMSE=1.0381, Test_RMSE=1.0584\n",
      "Epoch 357: Loss=0.6076, Train_RMSE=1.0374, Test_RMSE=1.0577\n",
      "Epoch 358: Loss=0.6068, Train_RMSE=1.0368, Test_RMSE=1.0570\n",
      "Epoch 359: Loss=0.6080, Train_RMSE=1.0361, Test_RMSE=1.0563\n",
      "Epoch 360: Loss=0.6070, Train_RMSE=1.0355, Test_RMSE=1.0557\n",
      "Epoch 361: Loss=0.6066, Train_RMSE=1.0348, Test_RMSE=1.0550\n",
      "Epoch 362: Loss=0.6068, Train_RMSE=1.0342, Test_RMSE=1.0544\n",
      "Epoch 363: Loss=0.6049, Train_RMSE=1.0335, Test_RMSE=1.0538\n",
      "Epoch 364: Loss=0.6066, Train_RMSE=1.0329, Test_RMSE=1.0531\n",
      "Epoch 365: Loss=0.6046, Train_RMSE=1.0323, Test_RMSE=1.0525\n",
      "Epoch 366: Loss=0.6054, Train_RMSE=1.0317, Test_RMSE=1.0519\n",
      "Epoch 367: Loss=0.6056, Train_RMSE=1.0311, Test_RMSE=1.0513\n",
      "Epoch 368: Loss=0.6058, Train_RMSE=1.0305, Test_RMSE=1.0507\n",
      "Epoch 369: Loss=0.6052, Train_RMSE=1.0299, Test_RMSE=1.0501\n",
      "Epoch 370: Loss=0.6055, Train_RMSE=1.0293, Test_RMSE=1.0495\n",
      "Epoch 371: Loss=0.6065, Train_RMSE=1.0287, Test_RMSE=1.0489\n",
      "Epoch 372: Loss=0.6061, Train_RMSE=1.0281, Test_RMSE=1.0483\n",
      "Epoch 373: Loss=0.6052, Train_RMSE=1.0275, Test_RMSE=1.0477\n",
      "Epoch 374: Loss=0.6040, Train_RMSE=1.0269, Test_RMSE=1.0471\n",
      "Epoch 375: Loss=0.6038, Train_RMSE=1.0264, Test_RMSE=1.0466\n",
      "Epoch 376: Loss=0.6031, Train_RMSE=1.0258, Test_RMSE=1.0460\n",
      "Epoch 377: Loss=0.6029, Train_RMSE=1.0252, Test_RMSE=1.0454\n",
      "Epoch 378: Loss=0.6039, Train_RMSE=1.0247, Test_RMSE=1.0449\n",
      "Epoch 379: Loss=0.6031, Train_RMSE=1.0241, Test_RMSE=1.0443\n",
      "Epoch 380: Loss=0.6029, Train_RMSE=1.0236, Test_RMSE=1.0438\n",
      "Epoch 381: Loss=0.6045, Train_RMSE=1.0230, Test_RMSE=1.0432\n",
      "Epoch 382: Loss=0.6016, Train_RMSE=1.0225, Test_RMSE=1.0427\n",
      "Epoch 383: Loss=0.6020, Train_RMSE=1.0220, Test_RMSE=1.0422\n",
      "Epoch 384: Loss=0.6029, Train_RMSE=1.0215, Test_RMSE=1.0416\n",
      "Epoch 385: Loss=0.6049, Train_RMSE=1.0210, Test_RMSE=1.0411\n",
      "Epoch 386: Loss=0.6030, Train_RMSE=1.0205, Test_RMSE=1.0406\n",
      "Epoch 387: Loss=0.6029, Train_RMSE=1.0200, Test_RMSE=1.0401\n",
      "Epoch 388: Loss=0.6015, Train_RMSE=1.0195, Test_RMSE=1.0396\n",
      "Epoch 389: Loss=0.6038, Train_RMSE=1.0190, Test_RMSE=1.0391\n",
      "Epoch 390: Loss=0.6038, Train_RMSE=1.0185, Test_RMSE=1.0386\n",
      "Epoch 391: Loss=0.6015, Train_RMSE=1.0181, Test_RMSE=1.0382\n",
      "Epoch 392: Loss=0.6020, Train_RMSE=1.0176, Test_RMSE=1.0377\n",
      "Epoch 393: Loss=0.6005, Train_RMSE=1.0171, Test_RMSE=1.0372\n",
      "Epoch 394: Loss=0.6023, Train_RMSE=1.0166, Test_RMSE=1.0367\n",
      "Epoch 395: Loss=0.6026, Train_RMSE=1.0162, Test_RMSE=1.0363\n",
      "Epoch 396: Loss=0.6021, Train_RMSE=1.0157, Test_RMSE=1.0358\n",
      "Epoch 397: Loss=0.6024, Train_RMSE=1.0153, Test_RMSE=1.0353\n",
      "Epoch 398: Loss=0.6006, Train_RMSE=1.0148, Test_RMSE=1.0349\n",
      "Epoch 399: Loss=0.6022, Train_RMSE=1.0144, Test_RMSE=1.0344\n",
      "Epoch 400: Loss=0.6032, Train_RMSE=1.0139, Test_RMSE=1.0340\n",
      "LightGCN+Attention - Test RMSE: 1.0340, Recall@10: 0.0474\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, edge_index, train_user, train_item, train_rating, \n",
    "                test_user, test_item, test_rating, \n",
    "                num_items, alpha=0.2, epochs=50, lr=0.001, weight_decay=1e-5):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_rmse = float('inf')\n",
    "    best_state = None\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        loss = train_one_epoch(model, optimizer, edge_index, train_user, train_item, train_rating, num_items, alpha)\n",
    "        tr_rmse = evaluate_rmse(model, edge_index, train_user, train_item, train_rating)\n",
    "        val_rmse = evaluate_rmse(model, edge_index, test_user, test_item, test_rating)\n",
    "        \n",
    "        print(f\"Epoch {epoch:03d}: Loss={loss:.4f}, Train_RMSE={tr_rmse:.4f}, Test_RMSE={val_rmse:.4f}\")\n",
    "\n",
    "        # Early stopping on validation RMSE\n",
    "        if val_rmse < best_rmse:\n",
    "            best_rmse = val_rmse\n",
    "            best_state = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter > patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Train the baseline model\n",
    "baseline_model = LightGCN(num_nodes=num_nodes, embedding_dim=64, num_layers=3).to(device)\n",
    "print(\"Training Baseline LightGCN...\")\n",
    "baseline_model = train_model(baseline_model, data.edge_index, train_user, train_item, train_rating, \n",
    "                             test_user, test_item, test_rating, num_items, alpha=0.3)\n",
    "\n",
    "baseline_rmse = evaluate_rmse(baseline_model, data.edge_index, test_user, test_item, test_rating)\n",
    "baseline_recall = recall_at_k(baseline_model, k=10)\n",
    "print(f\"Baseline LightGCN - Test RMSE: {baseline_rmse:.4f}, Recall@10: {baseline_recall:.4f}\")\n",
    "\n",
    "# Train the LightGCN with Attention model\n",
    "att_model = LightGCNWithAttention(num_nodes=num_nodes, embedding_dim=128, num_layers=3, dropout=0.4).to(device)\n",
    "print(\"\\nTraining LightGCN with Attention...\")\n",
    "att_model = train_model(att_model, data.edge_index, train_user, train_item, train_rating, \n",
    "                        test_user, test_item, test_rating, num_items, alpha=0.2, \n",
    "                        epochs=400, lr=0.0005, weight_decay=1e-4)\n",
    "\n",
    "att_rmse = evaluate_rmse(att_model, data.edge_index, test_user, test_item, test_rating)\n",
    "att_recall = recall_at_k(att_model, k=10)\n",
    "print(f\"LightGCN+Attention - Test RMSE: {att_rmse:.4f}, Recall@10: {att_recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 12: Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Final Comparison =====\n",
      "Baseline LightGCN    : RMSE = 1.3485, Recall@10 = 0.0868\n",
      "LightGCN + Attention : RMSE = 1.0340, Recall@10 = 0.0474\n"
     ]
    }
   ],
   "source": [
    "print(\"===== Final Comparison =====\")\n",
    "print(f\"Baseline LightGCN    : RMSE = {baseline_rmse:.4f}, Recall@10 = {baseline_recall:.4f}\")\n",
    "print(f\"LightGCN + Attention : RMSE = {att_rmse:.4f}, Recall@10 = {att_recall:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

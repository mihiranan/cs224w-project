{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1: Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.20.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torchvision) (2.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\renee\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch-geometric==2.3.0 in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.3.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch-geometric==2.3.0) (4.67.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch-geometric==2.3.0) (2.1.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch-geometric==2.3.0) (1.14.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch-geometric==2.3.0) (3.1.4)\n",
      "Requirement already satisfied: requests in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch-geometric==2.3.0) (2.32.3)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch-geometric==2.3.0) (3.2.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch-geometric==2.3.0) (1.5.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch-geometric==2.3.0) (6.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->torch-geometric==2.3.0) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->torch-geometric==2.3.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->torch-geometric==2.3.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->torch-geometric==2.3.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->torch-geometric==2.3.0) (2024.8.30)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn->torch-geometric==2.3.0) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn->torch-geometric==2.3.0) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm->torch-geometric==2.3.0) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\renee\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.1.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\renee\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\renee\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'import torch; print(torch.__version__)).html'\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\renee\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install torch-geometric==2.3.0\n",
    "!pip install pandas numpy scikit-learn\n",
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-$(python -c \"import torch; print(torch.__version__)\").html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import degree\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_scatter import scatter_softmax\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3: Setting Device and Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# For reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4: Load and Preprocess the MovieLens Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 610\n",
      "Number of items: 9724\n",
      "Number of total nodes: 10334\n",
      "Number of interactions: 100836\n"
     ]
    }
   ],
   "source": [
    "# Ensure the dataset is in the working directory: 'ml-latest-small/ratings.csv' and 'movies.csv'\n",
    "ratings = pd.read_csv('ml-latest-small/ratings.csv')\n",
    "movies = pd.read_csv('ml-latest-small/movies.csv')\n",
    "\n",
    "# Filter out any rows with missing userIds (shouldn't happen, but just in case)\n",
    "ratings = ratings[ratings['userId'].notna()]\n",
    "\n",
    "# Map user and movie IDs to consecutive integers\n",
    "user_id_mapping = {id: idx for idx, id in enumerate(ratings['userId'].unique())}\n",
    "item_id_mapping = {id: idx for idx, id in enumerate(ratings['movieId'].unique())}\n",
    "\n",
    "ratings['userId'] = ratings['userId'].map(user_id_mapping)\n",
    "ratings['movieId'] = ratings['movieId'].map(item_id_mapping)\n",
    "\n",
    "num_users = ratings['userId'].nunique()\n",
    "num_items = ratings['movieId'].nunique()\n",
    "num_nodes = num_users + num_items\n",
    "\n",
    "print(\"Number of users:\", num_users)\n",
    "print(\"Number of items:\", num_items)\n",
    "print(\"Number of total nodes:\", num_nodes)\n",
    "print(\"Number of interactions:\", len(ratings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 5: Create Graph Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create edge index for the entire dataset\n",
    "# Users are [0, num_users-1], Items are [num_users, num_users+num_items-1]\n",
    "user_nodes = ratings['userId'].to_numpy()\n",
    "item_nodes = ratings['movieId'].to_numpy() + num_users\n",
    "\n",
    "edge_index = np.vstack((user_nodes, item_nodes))\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "\n",
    "# Edge attributes are the ratings\n",
    "edge_attr = torch.tensor(ratings['rating'].to_numpy(), dtype=torch.float32)\n",
    "\n",
    "# Feature matrix: we can start with a simple identity or zero embeddings, as LightGCN learns embeddings directly\n",
    "# We'll rely solely on the learned embeddings from the model\n",
    "data = Data(edge_index=edge_index, num_nodes=num_nodes)\n",
    "\n",
    "# Move to device\n",
    "data = data.to(device)\n",
    "edge_attr = edge_attr.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 6: Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "train_user = torch.tensor(train_data['userId'].values, dtype=torch.long, device=device)\n",
    "train_item = torch.tensor(train_data['movieId'].values + num_users, dtype=torch.long, device=device)\n",
    "train_rating = torch.tensor(train_data['rating'].values, dtype=torch.float32, device=device)\n",
    "\n",
    "test_user = torch.tensor(test_data['userId'].values, dtype=torch.long, device=device)\n",
    "test_item = torch.tensor(test_data['movieId'].values + num_users, dtype=torch.long, device=device)\n",
    "test_rating = torch.tensor(test_data['rating'].values, dtype=torch.float32, device=device)\n",
    "\n",
    "# For Recall@K calculation, we will need the test edges separately\n",
    "test_edge_index = torch.stack([test_user, test_item], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 7: Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(true_ratings, pred_ratings):\n",
    "    return np.sqrt(mean_squared_error(true_ratings, pred_ratings))\n",
    "\n",
    "def recall_at_k(model, k=10):\n",
    "    \"\"\"\n",
    "    Compute Recall@K on the test set:\n",
    "    - We consider all items and see if the items the user actually interacted with (in test set)\n",
    "      appear in the top K recommendations for that user.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get embeddings\n",
    "        embeddings = model.get_embedding(data.edge_index)\n",
    "        user_emb = embeddings[:num_users]\n",
    "        item_emb = embeddings[num_users:num_users+num_items]\n",
    "\n",
    "        # Compute scores [num_users x num_items]\n",
    "        scores = user_emb @ item_emb.T\n",
    "\n",
    "        # Get top-k items for each user\n",
    "        _, top_k_items = torch.topk(scores, k, dim=1)\n",
    "\n",
    "        # Convert test set into a dict: user -> set of test items\n",
    "        test_user_items = {}\n",
    "        for u, i, r in zip(test_data['userId'], test_data['movieId'], test_data['rating']):\n",
    "            # Only consider items where user interacted positively (rating > 0)\n",
    "            # In MovieLens all ratings > 0 by definition, but we keep the check for generality\n",
    "            if u not in test_user_items:\n",
    "                test_user_items[u] = set()\n",
    "            test_user_items[u].add(i)\n",
    "\n",
    "        recalls = []\n",
    "        for u in range(num_users):\n",
    "            if u in test_user_items and len(test_user_items[u]) > 0:\n",
    "                recommended = set((top_k_items[u].cpu().numpy()))\n",
    "                relevant = test_user_items[u]\n",
    "                hit_count = len(recommended & relevant)\n",
    "                recall_u = hit_count / len(relevant)\n",
    "                recalls.append(recall_u)\n",
    "            else:\n",
    "                # If a user has no test items, skip them or consider recall as 0\n",
    "                # Usually, we consider only users with test interactions\n",
    "                pass\n",
    "\n",
    "        if len(recalls) == 0:\n",
    "            return 0.0\n",
    "        return float(np.mean(recalls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 8: LightGCN Model (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGCNConv(MessagePassing):\n",
    "    def __init__(self):\n",
    "        super().__init__(aggr='add')\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        row, col = edge_index\n",
    "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
    "        deg_inv_sqrt = (deg + 1e-7).pow(-0.5)  # Add epsilon\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "class LightGCN(nn.Module):\n",
    "    def __init__(self, num_nodes, embedding_dim=64, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(num_nodes, embedding_dim)\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "\n",
    "        self.convs = nn.ModuleList([LightGCNConv() for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, edge_index):\n",
    "        x = self.embedding.weight\n",
    "        all_embeddings = [x]\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            all_embeddings.append(x)\n",
    "        # Mean of all layer embeddings\n",
    "        x = torch.mean(torch.stack(all_embeddings, dim=0), dim=0)\n",
    "        return x\n",
    "\n",
    "    def get_embedding(self, edge_index):\n",
    "        return self.forward(edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 9: LightGCN with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LightGCNConvWithAttention(MessagePassing):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__(aggr='add')\n",
    "        self.att = nn.Parameter(torch.Tensor(1, in_channels * 2))\n",
    "        nn.init.xavier_uniform_(self.att)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        return self.propagate(edge_index, x=x)\n",
    "\n",
    "    def message(self, x_i, x_j):\n",
    "        x_cat = torch.cat([x_i, x_j], dim=-1)\n",
    "        alpha = F.leaky_relu((x_cat * self.att).sum(dim=-1))\n",
    "        alpha = F.softmax(alpha, dim=0)\n",
    "        return alpha.unsqueeze(-1) * x_j\n",
    "\n",
    "\n",
    "\n",
    "class LightGCNWithAttention(nn.Module):\n",
    "    def __init__(self, num_nodes, embedding_dim=256, num_layers=4, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(num_nodes, embedding_dim)\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "            LightGCNConvWithAttention(embedding_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, edge_index):\n",
    "        x = self.embedding.weight\n",
    "        all_embeddings = [x]\n",
    "\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = self.layer_norm(x)\n",
    "            x = self.dropout(x)\n",
    "            all_embeddings.append(x)\n",
    "\n",
    "        return torch.stack(all_embeddings, dim=0).sum(dim=0)\n",
    "\n",
    "    def get_embedding(self, edge_index):\n",
    "        embeddings = self.forward(edge_index)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 10: Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpr_loss(user_emb, pos_item_emb, neg_item_emb):\n",
    "    pos_scores = (user_emb * pos_item_emb).sum(dim=1)\n",
    "    neg_scores = (user_emb * neg_item_emb).sum(dim=1)\n",
    "    loss = -torch.log(torch.sigmoid(pos_scores - neg_scores)).mean()\n",
    "    return loss\n",
    "\n",
    "def hybrid_loss(user_emb, pos_item_emb, neg_item_emb, pred_ratings, true_ratings, alpha=0.3):\n",
    "    # RMSE Loss\n",
    "    rmse_loss = F.mse_loss(pred_ratings, true_ratings)\n",
    "    \n",
    "    # BPR Loss\n",
    "    pos_scores = (user_emb * pos_item_emb).sum(dim=1)\n",
    "    neg_scores = (user_emb * neg_item_emb).sum(dim=1)\n",
    "    bpr_loss = -torch.log(torch.sigmoid(pos_scores - neg_scores)).mean()\n",
    "    \n",
    "    # Weighted combination\n",
    "    return alpha * rmse_loss + (1 - alpha) * bpr_loss\n",
    "\n",
    "\n",
    "\n",
    "def train_one_epoch(model, optimizer, edge_index, user, item, rating, num_items, alpha=0.3):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Get embeddings\n",
    "    embeddings = model.get_embedding(edge_index)\n",
    "    user_emb = embeddings[user]\n",
    "    pos_item_emb = embeddings[item]\n",
    "\n",
    "    # Negative sampling: randomly select negative items\n",
    "    neg_items = torch.randint(0, num_items, (len(user),), device=device)\n",
    "    neg_item_emb = embeddings[neg_items + num_users]\n",
    "\n",
    "    # Predict ratings for positive samples\n",
    "    pred_ratings = (user_emb * pos_item_emb).sum(dim=1)\n",
    "\n",
    "    # Calculate hybrid loss\n",
    "    loss = hybrid_loss(user_emb, pos_item_emb, neg_item_emb, pred_ratings, rating, alpha)\n",
    "\n",
    "    # Add L2 regularization (on embeddings)\n",
    "    l2_reg = 1e-4 * torch.norm(model.embedding.weight)\n",
    "    loss += l2_reg\n",
    "\n",
    "    # Add L2 regularization for attention (only for LightGCNWithAttention)\n",
    "    if hasattr(model, 'convs') and isinstance(model.convs[0], LightGCNConvWithAttention):\n",
    "        for conv in model.convs:\n",
    "            if hasattr(conv, 'att'):  # Check if 'att' exists\n",
    "                loss += 1e-5 * torch.norm(conv.att)\n",
    "\n",
    "    # Backpropagation and optimizer step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_rmse(model, edge_index, user, item, rating):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.get_embedding(edge_index)\n",
    "        user_emb = embeddings[user]\n",
    "        item_emb = embeddings[item]\n",
    "        pred = (user_emb * item_emb).sum(dim=1)\n",
    "        pred = torch.clamp(pred, min=0.0, max=5.0)\n",
    "        true = rating.cpu().numpy()\n",
    "        return rmse(true, pred.cpu().numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 11: Training Loops and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Baseline LightGCN...\n",
      "Epoch 001: Loss=3.5015, Train_RMSE=3.0432, Test_RMSE=3.0425, Train_Recall_at_10=0.0696\n",
      "Epoch 002: Loss=3.1533, Train_RMSE=2.7936, Test_RMSE=2.7960, Train_Recall_at_10=0.0803\n",
      "Epoch 003: Loss=2.6636, Train_RMSE=2.4618, Test_RMSE=2.4662, Train_Recall_at_10=0.0868\n",
      "Epoch 004: Loss=2.0859, Train_RMSE=2.0770, Test_RMSE=2.0811, Train_Recall_at_10=0.0798\n",
      "Epoch 005: Loss=1.5149, Train_RMSE=1.7159, Test_RMSE=1.7159, Train_Recall_at_10=0.0725\n",
      "Epoch 006: Loss=1.0965, Train_RMSE=1.4792, Test_RMSE=1.4755, Train_Recall_at_10=0.0693\n",
      "Epoch 007: Loss=0.9776, Train_RMSE=1.3841, Test_RMSE=1.3789, Train_Recall_at_10=0.0716\n",
      "Epoch 008: Loss=1.1054, Train_RMSE=1.3512, Test_RMSE=1.3469, Train_Recall_at_10=0.0717\n",
      "Epoch 009: Loss=1.2118, Train_RMSE=1.3198, Test_RMSE=1.3174, Train_Recall_at_10=0.0743\n",
      "Epoch 010: Loss=1.1462, Train_RMSE=1.2721, Test_RMSE=1.2723, Train_Recall_at_10=0.0746\n",
      "Epoch 011: Loss=0.9603, Train_RMSE=1.2152, Test_RMSE=1.2192, Train_Recall_at_10=0.0766\n",
      "Epoch 012: Loss=0.7688, Train_RMSE=1.1750, Test_RMSE=1.1830, Train_Recall_at_10=0.0773\n",
      "Epoch 013: Loss=0.6387, Train_RMSE=1.1782, Test_RMSE=1.1912, Train_Recall_at_10=0.0791\n",
      "Epoch 014: Loss=0.5918, Train_RMSE=1.2255, Test_RMSE=1.2418, Train_Recall_at_10=0.0853\n",
      "Epoch 015: Loss=0.6111, Train_RMSE=1.2902, Test_RMSE=1.3082, Train_Recall_at_10=0.0837\n",
      "Epoch 016: Loss=0.6565, Train_RMSE=1.3444, Test_RMSE=1.3633, Train_Recall_at_10=0.0839\n",
      "Epoch 017: Loss=0.7012, Train_RMSE=1.3730, Test_RMSE=1.3923, Train_Recall_at_10=0.0858\n",
      "Epoch 018: Loss=0.7261, Train_RMSE=1.3727, Test_RMSE=1.3926, Train_Recall_at_10=0.0839\n",
      "Early stopping triggered.\n",
      "Baseline LightGCN - Test RMSE: 1.3926, Recall@10: 0.0839\n",
      "\n",
      "Training LightGCN with Attention...\n",
      "Epoch 001: Loss=3.2259, Train_RMSE=3.6532, Test_RMSE=3.6507, Train_Recall_at_10=0.0005\n",
      "Epoch 002: Loss=3.2254, Train_RMSE=3.6529, Test_RMSE=3.6503, Train_Recall_at_10=0.0012\n",
      "Epoch 003: Loss=3.2245, Train_RMSE=3.6521, Test_RMSE=3.6496, Train_Recall_at_10=0.0026\n",
      "Epoch 004: Loss=3.2230, Train_RMSE=3.6509, Test_RMSE=3.6484, Train_Recall_at_10=0.0077\n",
      "Epoch 005: Loss=3.2210, Train_RMSE=3.6492, Test_RMSE=3.6468, Train_Recall_at_10=0.0157\n",
      "Epoch 006: Loss=3.2184, Train_RMSE=3.6470, Test_RMSE=3.6447, Train_Recall_at_10=0.0214\n",
      "Epoch 007: Loss=3.2150, Train_RMSE=3.6444, Test_RMSE=3.6421, Train_Recall_at_10=0.0275\n",
      "Epoch 008: Loss=3.2109, Train_RMSE=3.6413, Test_RMSE=3.6391, Train_Recall_at_10=0.0329\n",
      "Epoch 009: Loss=3.2061, Train_RMSE=3.6377, Test_RMSE=3.6355, Train_Recall_at_10=0.0397\n",
      "Epoch 010: Loss=3.2005, Train_RMSE=3.6336, Test_RMSE=3.6314, Train_Recall_at_10=0.0423\n",
      "Epoch 011: Loss=3.1942, Train_RMSE=3.6289, Test_RMSE=3.6268, Train_Recall_at_10=0.0447\n",
      "Epoch 012: Loss=3.1867, Train_RMSE=3.6237, Test_RMSE=3.6215, Train_Recall_at_10=0.0454\n",
      "Epoch 013: Loss=3.1792, Train_RMSE=3.6178, Test_RMSE=3.6157, Train_Recall_at_10=0.0472\n",
      "Epoch 014: Loss=3.1696, Train_RMSE=3.6113, Test_RMSE=3.6093, Train_Recall_at_10=0.0478\n",
      "Epoch 015: Loss=3.1594, Train_RMSE=3.6042, Test_RMSE=3.6023, Train_Recall_at_10=0.0496\n",
      "Epoch 016: Loss=3.1487, Train_RMSE=3.5964, Test_RMSE=3.5946, Train_Recall_at_10=0.0497\n",
      "Epoch 017: Loss=3.1369, Train_RMSE=3.5879, Test_RMSE=3.5862, Train_Recall_at_10=0.0504\n",
      "Epoch 018: Loss=3.1226, Train_RMSE=3.5788, Test_RMSE=3.5771, Train_Recall_at_10=0.0515\n",
      "Epoch 019: Loss=3.1095, Train_RMSE=3.5689, Test_RMSE=3.5673, Train_Recall_at_10=0.0514\n",
      "Epoch 020: Loss=3.0953, Train_RMSE=3.5582, Test_RMSE=3.5568, Train_Recall_at_10=0.0517\n",
      "Epoch 021: Loss=3.0779, Train_RMSE=3.5468, Test_RMSE=3.5455, Train_Recall_at_10=0.0516\n",
      "Epoch 022: Loss=3.0614, Train_RMSE=3.5347, Test_RMSE=3.5335, Train_Recall_at_10=0.0513\n",
      "Epoch 023: Loss=3.0434, Train_RMSE=3.5217, Test_RMSE=3.5206, Train_Recall_at_10=0.0517\n",
      "Epoch 024: Loss=3.0238, Train_RMSE=3.5079, Test_RMSE=3.5070, Train_Recall_at_10=0.0515\n",
      "Epoch 025: Loss=3.0027, Train_RMSE=3.4933, Test_RMSE=3.4925, Train_Recall_at_10=0.0520\n",
      "Epoch 026: Loss=2.9806, Train_RMSE=3.4778, Test_RMSE=3.4772, Train_Recall_at_10=0.0519\n",
      "Epoch 027: Loss=2.9590, Train_RMSE=3.4615, Test_RMSE=3.4611, Train_Recall_at_10=0.0519\n",
      "Epoch 028: Loss=2.9351, Train_RMSE=3.4443, Test_RMSE=3.4441, Train_Recall_at_10=0.0520\n",
      "Epoch 029: Loss=2.9086, Train_RMSE=3.4262, Test_RMSE=3.4262, Train_Recall_at_10=0.0520\n",
      "Epoch 030: Loss=2.8819, Train_RMSE=3.4072, Test_RMSE=3.4075, Train_Recall_at_10=0.0520\n",
      "Epoch 031: Loss=2.8501, Train_RMSE=3.3873, Test_RMSE=3.3878, Train_Recall_at_10=0.0520\n",
      "Epoch 032: Loss=2.8217, Train_RMSE=3.3665, Test_RMSE=3.3672, Train_Recall_at_10=0.0520\n",
      "Epoch 033: Loss=2.7962, Train_RMSE=3.3447, Test_RMSE=3.3457, Train_Recall_at_10=0.0520\n",
      "Epoch 034: Loss=2.7662, Train_RMSE=3.3220, Test_RMSE=3.3233, Train_Recall_at_10=0.0520\n",
      "Epoch 035: Loss=2.7321, Train_RMSE=3.2984, Test_RMSE=3.2999, Train_Recall_at_10=0.0519\n",
      "Epoch 036: Loss=2.6976, Train_RMSE=3.2738, Test_RMSE=3.2756, Train_Recall_at_10=0.0519\n",
      "Epoch 037: Loss=2.6672, Train_RMSE=3.2482, Test_RMSE=3.2504, Train_Recall_at_10=0.0518\n",
      "Epoch 038: Loss=2.6268, Train_RMSE=3.2217, Test_RMSE=3.2242, Train_Recall_at_10=0.0523\n",
      "Epoch 039: Loss=2.5877, Train_RMSE=3.1942, Test_RMSE=3.1971, Train_Recall_at_10=0.0523\n",
      "Epoch 040: Loss=2.5472, Train_RMSE=3.1658, Test_RMSE=3.1690, Train_Recall_at_10=0.0522\n",
      "Epoch 041: Loss=2.5118, Train_RMSE=3.1363, Test_RMSE=3.1399, Train_Recall_at_10=0.0518\n",
      "Epoch 042: Loss=2.4709, Train_RMSE=3.1060, Test_RMSE=3.1099, Train_Recall_at_10=0.0517\n",
      "Epoch 043: Loss=2.4361, Train_RMSE=3.0746, Test_RMSE=3.0790, Train_Recall_at_10=0.0519\n",
      "Epoch 044: Loss=2.3858, Train_RMSE=3.0423, Test_RMSE=3.0471, Train_Recall_at_10=0.0516\n",
      "Epoch 045: Loss=2.3473, Train_RMSE=3.0090, Test_RMSE=3.0143, Train_Recall_at_10=0.0516\n",
      "Epoch 046: Loss=2.3045, Train_RMSE=2.9748, Test_RMSE=2.9805, Train_Recall_at_10=0.0515\n",
      "Epoch 047: Loss=2.2589, Train_RMSE=2.9397, Test_RMSE=2.9459, Train_Recall_at_10=0.0519\n",
      "Epoch 048: Loss=2.2196, Train_RMSE=2.9037, Test_RMSE=2.9104, Train_Recall_at_10=0.0522\n",
      "Epoch 049: Loss=2.1768, Train_RMSE=2.8668, Test_RMSE=2.8740, Train_Recall_at_10=0.0523\n",
      "Epoch 050: Loss=2.1244, Train_RMSE=2.8290, Test_RMSE=2.8367, Train_Recall_at_10=0.0524\n",
      "Epoch 051: Loss=2.0846, Train_RMSE=2.7904, Test_RMSE=2.7987, Train_Recall_at_10=0.0522\n",
      "Epoch 052: Loss=2.0222, Train_RMSE=2.7510, Test_RMSE=2.7598, Train_Recall_at_10=0.0522\n",
      "Epoch 053: Loss=1.9820, Train_RMSE=2.7107, Test_RMSE=2.7201, Train_Recall_at_10=0.0522\n",
      "Epoch 054: Loss=1.9376, Train_RMSE=2.6697, Test_RMSE=2.6797, Train_Recall_at_10=0.0524\n",
      "Epoch 055: Loss=1.8823, Train_RMSE=2.6279, Test_RMSE=2.6385, Train_Recall_at_10=0.0525\n",
      "Epoch 056: Loss=1.8432, Train_RMSE=2.5854, Test_RMSE=2.5967, Train_Recall_at_10=0.0525\n",
      "Epoch 057: Loss=1.8015, Train_RMSE=2.5422, Test_RMSE=2.5542, Train_Recall_at_10=0.0525\n",
      "Epoch 058: Loss=1.7424, Train_RMSE=2.4984, Test_RMSE=2.5110, Train_Recall_at_10=0.0525\n",
      "Epoch 059: Loss=1.6990, Train_RMSE=2.4540, Test_RMSE=2.4673, Train_Recall_at_10=0.0525\n",
      "Epoch 060: Loss=1.6439, Train_RMSE=2.4090, Test_RMSE=2.4231, Train_Recall_at_10=0.0525\n",
      "Epoch 061: Loss=1.6029, Train_RMSE=2.3636, Test_RMSE=2.3784, Train_Recall_at_10=0.0525\n",
      "Epoch 062: Loss=1.5640, Train_RMSE=2.3177, Test_RMSE=2.3333, Train_Recall_at_10=0.0525\n",
      "Epoch 063: Loss=1.5263, Train_RMSE=2.2715, Test_RMSE=2.2878, Train_Recall_at_10=0.0525\n",
      "Epoch 064: Loss=1.4615, Train_RMSE=2.2249, Test_RMSE=2.2421, Train_Recall_at_10=0.0525\n",
      "Epoch 065: Loss=1.4192, Train_RMSE=2.1781, Test_RMSE=2.1961, Train_Recall_at_10=0.0525\n",
      "Epoch 066: Loss=1.3785, Train_RMSE=2.1311, Test_RMSE=2.1499, Train_Recall_at_10=0.0525\n",
      "Epoch 067: Loss=1.3338, Train_RMSE=2.0839, Test_RMSE=2.1036, Train_Recall_at_10=0.0525\n",
      "Epoch 068: Loss=1.2846, Train_RMSE=2.0368, Test_RMSE=2.0574, Train_Recall_at_10=0.0525\n",
      "Epoch 069: Loss=1.2441, Train_RMSE=1.9897, Test_RMSE=2.0111, Train_Recall_at_10=0.0525\n",
      "Epoch 070: Loss=1.2051, Train_RMSE=1.9427, Test_RMSE=1.9651, Train_Recall_at_10=0.0525\n",
      "Epoch 071: Loss=1.1759, Train_RMSE=1.8959, Test_RMSE=1.9192, Train_Recall_at_10=0.0525\n",
      "Epoch 072: Loss=1.1334, Train_RMSE=1.8495, Test_RMSE=1.8738, Train_Recall_at_10=0.0525\n",
      "Epoch 073: Loss=1.0892, Train_RMSE=1.8035, Test_RMSE=1.8287, Train_Recall_at_10=0.0525\n",
      "Epoch 074: Loss=1.0579, Train_RMSE=1.7580, Test_RMSE=1.7843, Train_Recall_at_10=0.0525\n",
      "Epoch 075: Loss=1.0222, Train_RMSE=1.7132, Test_RMSE=1.7404, Train_Recall_at_10=0.0525\n",
      "Epoch 076: Loss=0.9992, Train_RMSE=1.6691, Test_RMSE=1.6973, Train_Recall_at_10=0.0525\n",
      "Epoch 077: Loss=0.9547, Train_RMSE=1.6259, Test_RMSE=1.6551, Train_Recall_at_10=0.0525\n",
      "Epoch 078: Loss=0.9264, Train_RMSE=1.5837, Test_RMSE=1.6139, Train_Recall_at_10=0.0525\n",
      "Epoch 079: Loss=0.8888, Train_RMSE=1.5426, Test_RMSE=1.5738, Train_Recall_at_10=0.0525\n",
      "Epoch 080: Loss=0.8610, Train_RMSE=1.5027, Test_RMSE=1.5349, Train_Recall_at_10=0.0525\n",
      "Epoch 081: Loss=0.8422, Train_RMSE=1.4642, Test_RMSE=1.4973, Train_Recall_at_10=0.0525\n",
      "Epoch 082: Loss=0.8147, Train_RMSE=1.4271, Test_RMSE=1.4612, Train_Recall_at_10=0.0525\n",
      "Epoch 083: Loss=0.7852, Train_RMSE=1.3916, Test_RMSE=1.4266, Train_Recall_at_10=0.0525\n",
      "Epoch 084: Loss=0.7658, Train_RMSE=1.3578, Test_RMSE=1.3937, Train_Recall_at_10=0.0525\n",
      "Epoch 085: Loss=0.7447, Train_RMSE=1.3257, Test_RMSE=1.3625, Train_Recall_at_10=0.0525\n",
      "Epoch 086: Loss=0.7288, Train_RMSE=1.2954, Test_RMSE=1.3330, Train_Recall_at_10=0.0525\n",
      "Epoch 087: Loss=0.7111, Train_RMSE=1.2669, Test_RMSE=1.3054, Train_Recall_at_10=0.0525\n",
      "Epoch 088: Loss=0.6982, Train_RMSE=1.2404, Test_RMSE=1.2796, Train_Recall_at_10=0.0525\n",
      "Epoch 089: Loss=0.6776, Train_RMSE=1.2159, Test_RMSE=1.2557, Train_Recall_at_10=0.0525\n",
      "Epoch 090: Loss=0.6641, Train_RMSE=1.1933, Test_RMSE=1.2338, Train_Recall_at_10=0.0525\n",
      "Epoch 091: Loss=0.6528, Train_RMSE=1.1728, Test_RMSE=1.2138, Train_Recall_at_10=0.0525\n",
      "Epoch 092: Loss=0.6386, Train_RMSE=1.1542, Test_RMSE=1.1957, Train_Recall_at_10=0.0525\n",
      "Epoch 093: Loss=0.6346, Train_RMSE=1.1374, Test_RMSE=1.1793, Train_Recall_at_10=0.0525\n",
      "Epoch 094: Loss=0.6260, Train_RMSE=1.1223, Test_RMSE=1.1646, Train_Recall_at_10=0.0525\n",
      "Epoch 095: Loss=0.6132, Train_RMSE=1.1089, Test_RMSE=1.1515, Train_Recall_at_10=0.0525\n",
      "Epoch 096: Loss=0.6081, Train_RMSE=1.0970, Test_RMSE=1.1399, Train_Recall_at_10=0.0525\n",
      "Epoch 097: Loss=0.6052, Train_RMSE=1.0866, Test_RMSE=1.1298, Train_Recall_at_10=0.0525\n",
      "Epoch 098: Loss=0.5912, Train_RMSE=1.0776, Test_RMSE=1.1209, Train_Recall_at_10=0.0525\n",
      "Epoch 099: Loss=0.5878, Train_RMSE=1.0698, Test_RMSE=1.1133, Train_Recall_at_10=0.0525\n",
      "Epoch 100: Loss=0.5796, Train_RMSE=1.0631, Test_RMSE=1.1067, Train_Recall_at_10=0.0527\n",
      "Epoch 101: Loss=0.5816, Train_RMSE=1.0575, Test_RMSE=1.1012, Train_Recall_at_10=0.0527\n",
      "Epoch 102: Loss=0.5799, Train_RMSE=1.0527, Test_RMSE=1.0964, Train_Recall_at_10=0.0540\n",
      "Epoch 103: Loss=0.5778, Train_RMSE=1.0486, Test_RMSE=1.0925, Train_Recall_at_10=0.0542\n",
      "Epoch 104: Loss=0.5764, Train_RMSE=1.0452, Test_RMSE=1.0891, Train_Recall_at_10=0.0545\n",
      "Epoch 105: Loss=0.5747, Train_RMSE=1.0424, Test_RMSE=1.0863, Train_Recall_at_10=0.0549\n",
      "Epoch 106: Loss=0.5728, Train_RMSE=1.0400, Test_RMSE=1.0840, Train_Recall_at_10=0.0549\n",
      "Epoch 107: Loss=0.5677, Train_RMSE=1.0379, Test_RMSE=1.0819, Train_Recall_at_10=0.0548\n",
      "Epoch 108: Loss=0.5707, Train_RMSE=1.0361, Test_RMSE=1.0802, Train_Recall_at_10=0.0548\n",
      "Epoch 109: Loss=0.5657, Train_RMSE=1.0345, Test_RMSE=1.0787, Train_Recall_at_10=0.0548\n",
      "Epoch 110: Loss=0.5634, Train_RMSE=1.0331, Test_RMSE=1.0774, Train_Recall_at_10=0.0551\n",
      "Epoch 111: Loss=0.5633, Train_RMSE=1.0318, Test_RMSE=1.0761, Train_Recall_at_10=0.0523\n",
      "Epoch 112: Loss=0.5672, Train_RMSE=1.0305, Test_RMSE=1.0750, Train_Recall_at_10=0.0519\n",
      "Epoch 113: Loss=0.5636, Train_RMSE=1.0293, Test_RMSE=1.0739, Train_Recall_at_10=0.0519\n",
      "Epoch 114: Loss=0.5614, Train_RMSE=1.0280, Test_RMSE=1.0728, Train_Recall_at_10=0.0519\n",
      "Epoch 115: Loss=0.5587, Train_RMSE=1.0267, Test_RMSE=1.0717, Train_Recall_at_10=0.0519\n",
      "Epoch 116: Loss=0.5610, Train_RMSE=1.0254, Test_RMSE=1.0706, Train_Recall_at_10=0.0519\n",
      "Epoch 117: Loss=0.5612, Train_RMSE=1.0241, Test_RMSE=1.0694, Train_Recall_at_10=0.0519\n",
      "Epoch 118: Loss=0.5603, Train_RMSE=1.0227, Test_RMSE=1.0682, Train_Recall_at_10=0.0519\n",
      "Epoch 119: Loss=0.5615, Train_RMSE=1.0213, Test_RMSE=1.0670, Train_Recall_at_10=0.0519\n",
      "Epoch 120: Loss=0.5533, Train_RMSE=1.0198, Test_RMSE=1.0658, Train_Recall_at_10=0.0518\n",
      "Epoch 121: Loss=0.5560, Train_RMSE=1.0183, Test_RMSE=1.0645, Train_Recall_at_10=0.0518\n",
      "Epoch 122: Loss=0.5559, Train_RMSE=1.0167, Test_RMSE=1.0632, Train_Recall_at_10=0.0516\n",
      "Epoch 123: Loss=0.5556, Train_RMSE=1.0152, Test_RMSE=1.0619, Train_Recall_at_10=0.0517\n",
      "Epoch 124: Loss=0.5571, Train_RMSE=1.0135, Test_RMSE=1.0606, Train_Recall_at_10=0.0512\n",
      "Epoch 125: Loss=0.5547, Train_RMSE=1.0119, Test_RMSE=1.0592, Train_Recall_at_10=0.0517\n",
      "Epoch 126: Loss=0.5532, Train_RMSE=1.0102, Test_RMSE=1.0578, Train_Recall_at_10=0.0515\n",
      "Epoch 127: Loss=0.5563, Train_RMSE=1.0086, Test_RMSE=1.0565, Train_Recall_at_10=0.0515\n",
      "Epoch 128: Loss=0.5545, Train_RMSE=1.0070, Test_RMSE=1.0552, Train_Recall_at_10=0.0515\n",
      "Epoch 129: Loss=0.5561, Train_RMSE=1.0054, Test_RMSE=1.0539, Train_Recall_at_10=0.0515\n",
      "Epoch 130: Loss=0.5506, Train_RMSE=1.0037, Test_RMSE=1.0525, Train_Recall_at_10=0.0515\n",
      "Epoch 131: Loss=0.5495, Train_RMSE=1.0022, Test_RMSE=1.0513, Train_Recall_at_10=0.0515\n",
      "Epoch 132: Loss=0.5468, Train_RMSE=1.0006, Test_RMSE=1.0500, Train_Recall_at_10=0.0515\n",
      "Epoch 133: Loss=0.5492, Train_RMSE=0.9991, Test_RMSE=1.0488, Train_Recall_at_10=0.0515\n",
      "Epoch 134: Loss=0.5479, Train_RMSE=0.9977, Test_RMSE=1.0476, Train_Recall_at_10=0.0515\n",
      "Epoch 135: Loss=0.5452, Train_RMSE=0.9963, Test_RMSE=1.0465, Train_Recall_at_10=0.0515\n",
      "Epoch 136: Loss=0.5514, Train_RMSE=0.9948, Test_RMSE=1.0453, Train_Recall_at_10=0.0515\n",
      "Epoch 137: Loss=0.5506, Train_RMSE=0.9935, Test_RMSE=1.0442, Train_Recall_at_10=0.0515\n",
      "Epoch 138: Loss=0.5442, Train_RMSE=0.9921, Test_RMSE=1.0431, Train_Recall_at_10=0.0515\n",
      "Epoch 139: Loss=0.5453, Train_RMSE=0.9908, Test_RMSE=1.0420, Train_Recall_at_10=0.0515\n",
      "Epoch 140: Loss=0.5482, Train_RMSE=0.9896, Test_RMSE=1.0410, Train_Recall_at_10=0.0515\n",
      "Epoch 141: Loss=0.5440, Train_RMSE=0.9883, Test_RMSE=1.0400, Train_Recall_at_10=0.0515\n",
      "Epoch 142: Loss=0.5431, Train_RMSE=0.9872, Test_RMSE=1.0390, Train_Recall_at_10=0.0515\n",
      "Epoch 143: Loss=0.5460, Train_RMSE=0.9860, Test_RMSE=1.0380, Train_Recall_at_10=0.0515\n",
      "Epoch 144: Loss=0.5433, Train_RMSE=0.9848, Test_RMSE=1.0371, Train_Recall_at_10=0.0515\n",
      "Epoch 145: Loss=0.5472, Train_RMSE=0.9837, Test_RMSE=1.0362, Train_Recall_at_10=0.0515\n",
      "Epoch 146: Loss=0.5442, Train_RMSE=0.9826, Test_RMSE=1.0353, Train_Recall_at_10=0.0515\n",
      "Epoch 147: Loss=0.5423, Train_RMSE=0.9815, Test_RMSE=1.0344, Train_Recall_at_10=0.0515\n",
      "Epoch 148: Loss=0.5452, Train_RMSE=0.9805, Test_RMSE=1.0335, Train_Recall_at_10=0.0515\n",
      "Epoch 149: Loss=0.5397, Train_RMSE=0.9795, Test_RMSE=1.0327, Train_Recall_at_10=0.0515\n",
      "Epoch 150: Loss=0.5390, Train_RMSE=0.9785, Test_RMSE=1.0318, Train_Recall_at_10=0.0515\n",
      "Epoch 151: Loss=0.5438, Train_RMSE=0.9775, Test_RMSE=1.0310, Train_Recall_at_10=0.0515\n",
      "Epoch 152: Loss=0.5417, Train_RMSE=0.9765, Test_RMSE=1.0302, Train_Recall_at_10=0.0515\n",
      "Epoch 153: Loss=0.5426, Train_RMSE=0.9755, Test_RMSE=1.0294, Train_Recall_at_10=0.0515\n",
      "Epoch 154: Loss=0.5419, Train_RMSE=0.9746, Test_RMSE=1.0286, Train_Recall_at_10=0.0515\n",
      "Epoch 155: Loss=0.5409, Train_RMSE=0.9737, Test_RMSE=1.0278, Train_Recall_at_10=0.0515\n",
      "Epoch 156: Loss=0.5406, Train_RMSE=0.9728, Test_RMSE=1.0270, Train_Recall_at_10=0.0516\n",
      "Epoch 157: Loss=0.5414, Train_RMSE=0.9719, Test_RMSE=1.0263, Train_Recall_at_10=0.0519\n",
      "Epoch 158: Loss=0.5421, Train_RMSE=0.9710, Test_RMSE=1.0255, Train_Recall_at_10=0.0519\n",
      "Epoch 159: Loss=0.5403, Train_RMSE=0.9701, Test_RMSE=1.0248, Train_Recall_at_10=0.0514\n",
      "Epoch 160: Loss=0.5401, Train_RMSE=0.9693, Test_RMSE=1.0241, Train_Recall_at_10=0.0514\n",
      "Epoch 161: Loss=0.5343, Train_RMSE=0.9685, Test_RMSE=1.0234, Train_Recall_at_10=0.0519\n",
      "Epoch 162: Loss=0.5380, Train_RMSE=0.9676, Test_RMSE=1.0227, Train_Recall_at_10=0.0519\n",
      "Epoch 163: Loss=0.5377, Train_RMSE=0.9669, Test_RMSE=1.0220, Train_Recall_at_10=0.0519\n",
      "Epoch 164: Loss=0.5347, Train_RMSE=0.9661, Test_RMSE=1.0214, Train_Recall_at_10=0.0520\n",
      "Epoch 165: Loss=0.5372, Train_RMSE=0.9653, Test_RMSE=1.0207, Train_Recall_at_10=0.0520\n",
      "Epoch 166: Loss=0.5359, Train_RMSE=0.9646, Test_RMSE=1.0201, Train_Recall_at_10=0.0520\n",
      "Epoch 167: Loss=0.5366, Train_RMSE=0.9639, Test_RMSE=1.0195, Train_Recall_at_10=0.0520\n",
      "Epoch 168: Loss=0.5360, Train_RMSE=0.9632, Test_RMSE=1.0189, Train_Recall_at_10=0.0521\n",
      "Epoch 169: Loss=0.5363, Train_RMSE=0.9625, Test_RMSE=1.0183, Train_Recall_at_10=0.0521\n",
      "Epoch 170: Loss=0.5338, Train_RMSE=0.9618, Test_RMSE=1.0177, Train_Recall_at_10=0.0520\n",
      "Epoch 171: Loss=0.5372, Train_RMSE=0.9612, Test_RMSE=1.0171, Train_Recall_at_10=0.0519\n",
      "Epoch 172: Loss=0.5342, Train_RMSE=0.9605, Test_RMSE=1.0166, Train_Recall_at_10=0.0511\n",
      "Epoch 173: Loss=0.5329, Train_RMSE=0.9599, Test_RMSE=1.0160, Train_Recall_at_10=0.0516\n",
      "Epoch 174: Loss=0.5321, Train_RMSE=0.9593, Test_RMSE=1.0155, Train_Recall_at_10=0.0502\n",
      "Epoch 175: Loss=0.5356, Train_RMSE=0.9587, Test_RMSE=1.0149, Train_Recall_at_10=0.0502\n",
      "Epoch 176: Loss=0.5356, Train_RMSE=0.9581, Test_RMSE=1.0144, Train_Recall_at_10=0.0500\n",
      "Epoch 177: Loss=0.5335, Train_RMSE=0.9575, Test_RMSE=1.0139, Train_Recall_at_10=0.0500\n",
      "Epoch 178: Loss=0.5339, Train_RMSE=0.9570, Test_RMSE=1.0134, Train_Recall_at_10=0.0500\n",
      "Epoch 179: Loss=0.5323, Train_RMSE=0.9564, Test_RMSE=1.0129, Train_Recall_at_10=0.0500\n",
      "Epoch 180: Loss=0.5325, Train_RMSE=0.9559, Test_RMSE=1.0125, Train_Recall_at_10=0.0500\n",
      "Epoch 181: Loss=0.5320, Train_RMSE=0.9553, Test_RMSE=1.0120, Train_Recall_at_10=0.0500\n",
      "Epoch 182: Loss=0.5348, Train_RMSE=0.9548, Test_RMSE=1.0115, Train_Recall_at_10=0.0500\n",
      "Epoch 183: Loss=0.5345, Train_RMSE=0.9543, Test_RMSE=1.0110, Train_Recall_at_10=0.0500\n",
      "Epoch 184: Loss=0.5327, Train_RMSE=0.9537, Test_RMSE=1.0105, Train_Recall_at_10=0.0500\n",
      "Epoch 185: Loss=0.5335, Train_RMSE=0.9531, Test_RMSE=1.0100, Train_Recall_at_10=0.0500\n",
      "Epoch 186: Loss=0.5321, Train_RMSE=0.9526, Test_RMSE=1.0095, Train_Recall_at_10=0.0500\n",
      "Epoch 187: Loss=0.5309, Train_RMSE=0.9521, Test_RMSE=1.0091, Train_Recall_at_10=0.0500\n",
      "Epoch 188: Loss=0.5296, Train_RMSE=0.9516, Test_RMSE=1.0086, Train_Recall_at_10=0.0500\n",
      "Epoch 189: Loss=0.5295, Train_RMSE=0.9511, Test_RMSE=1.0082, Train_Recall_at_10=0.0500\n",
      "Epoch 190: Loss=0.5324, Train_RMSE=0.9506, Test_RMSE=1.0079, Train_Recall_at_10=0.0500\n",
      "Epoch 191: Loss=0.5310, Train_RMSE=0.9502, Test_RMSE=1.0075, Train_Recall_at_10=0.0502\n",
      "Epoch 192: Loss=0.5320, Train_RMSE=0.9498, Test_RMSE=1.0071, Train_Recall_at_10=0.0503\n",
      "Epoch 193: Loss=0.5337, Train_RMSE=0.9493, Test_RMSE=1.0067, Train_Recall_at_10=0.0505\n",
      "Epoch 194: Loss=0.5300, Train_RMSE=0.9489, Test_RMSE=1.0063, Train_Recall_at_10=0.0507\n",
      "Epoch 195: Loss=0.5290, Train_RMSE=0.9484, Test_RMSE=1.0060, Train_Recall_at_10=0.0507\n",
      "Epoch 196: Loss=0.5317, Train_RMSE=0.9480, Test_RMSE=1.0056, Train_Recall_at_10=0.0507\n",
      "Epoch 197: Loss=0.5287, Train_RMSE=0.9476, Test_RMSE=1.0052, Train_Recall_at_10=0.0507\n",
      "Epoch 198: Loss=0.5314, Train_RMSE=0.9471, Test_RMSE=1.0049, Train_Recall_at_10=0.0507\n",
      "Epoch 199: Loss=0.5296, Train_RMSE=0.9467, Test_RMSE=1.0045, Train_Recall_at_10=0.0507\n",
      "Epoch 200: Loss=0.5308, Train_RMSE=0.9463, Test_RMSE=1.0042, Train_Recall_at_10=0.0507\n",
      "Epoch 201: Loss=0.5303, Train_RMSE=0.9459, Test_RMSE=1.0038, Train_Recall_at_10=0.0507\n",
      "Epoch 202: Loss=0.5304, Train_RMSE=0.9455, Test_RMSE=1.0035, Train_Recall_at_10=0.0507\n",
      "Epoch 203: Loss=0.5293, Train_RMSE=0.9451, Test_RMSE=1.0032, Train_Recall_at_10=0.0507\n",
      "Epoch 204: Loss=0.5293, Train_RMSE=0.9447, Test_RMSE=1.0029, Train_Recall_at_10=0.0507\n",
      "Epoch 205: Loss=0.5270, Train_RMSE=0.9443, Test_RMSE=1.0026, Train_Recall_at_10=0.0507\n",
      "Epoch 206: Loss=0.5282, Train_RMSE=0.9440, Test_RMSE=1.0023, Train_Recall_at_10=0.0507\n",
      "Epoch 207: Loss=0.5287, Train_RMSE=0.9436, Test_RMSE=1.0020, Train_Recall_at_10=0.0507\n",
      "Epoch 208: Loss=0.5304, Train_RMSE=0.9433, Test_RMSE=1.0017, Train_Recall_at_10=0.0507\n",
      "Epoch 209: Loss=0.5272, Train_RMSE=0.9429, Test_RMSE=1.0015, Train_Recall_at_10=0.0507\n",
      "Epoch 210: Loss=0.5286, Train_RMSE=0.9426, Test_RMSE=1.0012, Train_Recall_at_10=0.0507\n",
      "Epoch 211: Loss=0.5275, Train_RMSE=0.9423, Test_RMSE=1.0009, Train_Recall_at_10=0.0507\n",
      "Epoch 212: Loss=0.5300, Train_RMSE=0.9419, Test_RMSE=1.0007, Train_Recall_at_10=0.0507\n",
      "Epoch 213: Loss=0.5306, Train_RMSE=0.9416, Test_RMSE=1.0004, Train_Recall_at_10=0.0507\n",
      "Epoch 214: Loss=0.5292, Train_RMSE=0.9413, Test_RMSE=1.0001, Train_Recall_at_10=0.0507\n",
      "Epoch 215: Loss=0.5282, Train_RMSE=0.9410, Test_RMSE=0.9999, Train_Recall_at_10=0.0506\n",
      "Epoch 216: Loss=0.5282, Train_RMSE=0.9407, Test_RMSE=0.9997, Train_Recall_at_10=0.0506\n",
      "Epoch 217: Loss=0.5275, Train_RMSE=0.9404, Test_RMSE=0.9994, Train_Recall_at_10=0.0507\n",
      "Epoch 218: Loss=0.5288, Train_RMSE=0.9401, Test_RMSE=0.9992, Train_Recall_at_10=0.0517\n",
      "Epoch 219: Loss=0.5262, Train_RMSE=0.9398, Test_RMSE=0.9989, Train_Recall_at_10=0.0518\n",
      "Epoch 220: Loss=0.5294, Train_RMSE=0.9396, Test_RMSE=0.9987, Train_Recall_at_10=0.0521\n",
      "Epoch 221: Loss=0.5298, Train_RMSE=0.9393, Test_RMSE=0.9985, Train_Recall_at_10=0.0513\n",
      "Epoch 222: Loss=0.5289, Train_RMSE=0.9390, Test_RMSE=0.9983, Train_Recall_at_10=0.0512\n",
      "Epoch 223: Loss=0.5295, Train_RMSE=0.9387, Test_RMSE=0.9981, Train_Recall_at_10=0.0511\n",
      "Epoch 224: Loss=0.5293, Train_RMSE=0.9385, Test_RMSE=0.9978, Train_Recall_at_10=0.0511\n",
      "Epoch 225: Loss=0.5304, Train_RMSE=0.9382, Test_RMSE=0.9976, Train_Recall_at_10=0.0511\n",
      "Epoch 226: Loss=0.5272, Train_RMSE=0.9379, Test_RMSE=0.9974, Train_Recall_at_10=0.0511\n",
      "Epoch 227: Loss=0.5260, Train_RMSE=0.9377, Test_RMSE=0.9972, Train_Recall_at_10=0.0511\n",
      "Epoch 228: Loss=0.5278, Train_RMSE=0.9374, Test_RMSE=0.9970, Train_Recall_at_10=0.0511\n",
      "Epoch 229: Loss=0.5280, Train_RMSE=0.9372, Test_RMSE=0.9968, Train_Recall_at_10=0.0511\n",
      "Epoch 230: Loss=0.5279, Train_RMSE=0.9369, Test_RMSE=0.9966, Train_Recall_at_10=0.0511\n",
      "Epoch 231: Loss=0.5266, Train_RMSE=0.9366, Test_RMSE=0.9964, Train_Recall_at_10=0.0511\n",
      "Epoch 232: Loss=0.5261, Train_RMSE=0.9364, Test_RMSE=0.9962, Train_Recall_at_10=0.0511\n",
      "Epoch 233: Loss=0.5273, Train_RMSE=0.9362, Test_RMSE=0.9960, Train_Recall_at_10=0.0511\n",
      "Epoch 234: Loss=0.5273, Train_RMSE=0.9360, Test_RMSE=0.9958, Train_Recall_at_10=0.0511\n",
      "Epoch 235: Loss=0.5280, Train_RMSE=0.9357, Test_RMSE=0.9957, Train_Recall_at_10=0.0511\n",
      "Epoch 236: Loss=0.5267, Train_RMSE=0.9355, Test_RMSE=0.9955, Train_Recall_at_10=0.0511\n",
      "Epoch 237: Loss=0.5279, Train_RMSE=0.9353, Test_RMSE=0.9953, Train_Recall_at_10=0.0511\n",
      "Epoch 238: Loss=0.5269, Train_RMSE=0.9351, Test_RMSE=0.9951, Train_Recall_at_10=0.0511\n",
      "Epoch 239: Loss=0.5286, Train_RMSE=0.9349, Test_RMSE=0.9949, Train_Recall_at_10=0.0511\n",
      "Epoch 240: Loss=0.5248, Train_RMSE=0.9347, Test_RMSE=0.9948, Train_Recall_at_10=0.0511\n",
      "Epoch 241: Loss=0.5267, Train_RMSE=0.9345, Test_RMSE=0.9946, Train_Recall_at_10=0.0511\n",
      "Epoch 242: Loss=0.5266, Train_RMSE=0.9343, Test_RMSE=0.9944, Train_Recall_at_10=0.0511\n",
      "Epoch 243: Loss=0.5278, Train_RMSE=0.9341, Test_RMSE=0.9942, Train_Recall_at_10=0.0511\n",
      "Epoch 244: Loss=0.5260, Train_RMSE=0.9339, Test_RMSE=0.9940, Train_Recall_at_10=0.0511\n",
      "Epoch 245: Loss=0.5268, Train_RMSE=0.9337, Test_RMSE=0.9939, Train_Recall_at_10=0.0514\n",
      "Epoch 246: Loss=0.5264, Train_RMSE=0.9335, Test_RMSE=0.9937, Train_Recall_at_10=0.0516\n",
      "Epoch 247: Loss=0.5262, Train_RMSE=0.9334, Test_RMSE=0.9935, Train_Recall_at_10=0.0516\n",
      "Epoch 248: Loss=0.5268, Train_RMSE=0.9332, Test_RMSE=0.9934, Train_Recall_at_10=0.0506\n",
      "Epoch 249: Loss=0.5263, Train_RMSE=0.9330, Test_RMSE=0.9932, Train_Recall_at_10=0.0500\n",
      "Epoch 250: Loss=0.5259, Train_RMSE=0.9329, Test_RMSE=0.9931, Train_Recall_at_10=0.0505\n",
      "Epoch 251: Loss=0.5270, Train_RMSE=0.9327, Test_RMSE=0.9929, Train_Recall_at_10=0.0495\n",
      "Epoch 252: Loss=0.5253, Train_RMSE=0.9325, Test_RMSE=0.9927, Train_Recall_at_10=0.0508\n",
      "Epoch 253: Loss=0.5261, Train_RMSE=0.9323, Test_RMSE=0.9926, Train_Recall_at_10=0.0498\n",
      "Epoch 254: Loss=0.5271, Train_RMSE=0.9321, Test_RMSE=0.9924, Train_Recall_at_10=0.0508\n",
      "Epoch 255: Loss=0.5269, Train_RMSE=0.9319, Test_RMSE=0.9923, Train_Recall_at_10=0.0505\n",
      "Epoch 256: Loss=0.5266, Train_RMSE=0.9318, Test_RMSE=0.9921, Train_Recall_at_10=0.0511\n",
      "Epoch 257: Loss=0.5279, Train_RMSE=0.9316, Test_RMSE=0.9920, Train_Recall_at_10=0.0507\n",
      "Epoch 258: Loss=0.5265, Train_RMSE=0.9314, Test_RMSE=0.9918, Train_Recall_at_10=0.0504\n",
      "Epoch 259: Loss=0.5260, Train_RMSE=0.9313, Test_RMSE=0.9917, Train_Recall_at_10=0.0511\n",
      "Epoch 260: Loss=0.5267, Train_RMSE=0.9311, Test_RMSE=0.9916, Train_Recall_at_10=0.0511\n",
      "Epoch 261: Loss=0.5253, Train_RMSE=0.9309, Test_RMSE=0.9915, Train_Recall_at_10=0.0511\n",
      "Epoch 262: Loss=0.5276, Train_RMSE=0.9308, Test_RMSE=0.9913, Train_Recall_at_10=0.0511\n",
      "Epoch 263: Loss=0.5265, Train_RMSE=0.9306, Test_RMSE=0.9912, Train_Recall_at_10=0.0511\n",
      "Epoch 264: Loss=0.5267, Train_RMSE=0.9305, Test_RMSE=0.9911, Train_Recall_at_10=0.0511\n",
      "Epoch 265: Loss=0.5261, Train_RMSE=0.9303, Test_RMSE=0.9910, Train_Recall_at_10=0.0511\n",
      "Epoch 266: Loss=0.5247, Train_RMSE=0.9302, Test_RMSE=0.9909, Train_Recall_at_10=0.0511\n",
      "Epoch 267: Loss=0.5271, Train_RMSE=0.9300, Test_RMSE=0.9908, Train_Recall_at_10=0.0511\n",
      "Epoch 268: Loss=0.5258, Train_RMSE=0.9299, Test_RMSE=0.9906, Train_Recall_at_10=0.0511\n",
      "Epoch 269: Loss=0.5270, Train_RMSE=0.9298, Test_RMSE=0.9905, Train_Recall_at_10=0.0511\n",
      "Epoch 270: Loss=0.5249, Train_RMSE=0.9296, Test_RMSE=0.9904, Train_Recall_at_10=0.0511\n",
      "Epoch 271: Loss=0.5258, Train_RMSE=0.9295, Test_RMSE=0.9903, Train_Recall_at_10=0.0511\n",
      "Epoch 272: Loss=0.5258, Train_RMSE=0.9294, Test_RMSE=0.9902, Train_Recall_at_10=0.0511\n",
      "Epoch 273: Loss=0.5251, Train_RMSE=0.9292, Test_RMSE=0.9901, Train_Recall_at_10=0.0511\n",
      "Epoch 274: Loss=0.5264, Train_RMSE=0.9291, Test_RMSE=0.9900, Train_Recall_at_10=0.0511\n",
      "Epoch 275: Loss=0.5282, Train_RMSE=0.9290, Test_RMSE=0.9899, Train_Recall_at_10=0.0511\n",
      "Epoch 276: Loss=0.5251, Train_RMSE=0.9288, Test_RMSE=0.9898, Train_Recall_at_10=0.0511\n",
      "Epoch 277: Loss=0.5261, Train_RMSE=0.9287, Test_RMSE=0.9896, Train_Recall_at_10=0.0511\n",
      "Epoch 278: Loss=0.5255, Train_RMSE=0.9286, Test_RMSE=0.9895, Train_Recall_at_10=0.0511\n",
      "Epoch 279: Loss=0.5255, Train_RMSE=0.9284, Test_RMSE=0.9894, Train_Recall_at_10=0.0512\n",
      "Epoch 280: Loss=0.5249, Train_RMSE=0.9283, Test_RMSE=0.9893, Train_Recall_at_10=0.0513\n",
      "Epoch 281: Loss=0.5258, Train_RMSE=0.9281, Test_RMSE=0.9892, Train_Recall_at_10=0.0517\n",
      "Epoch 282: Loss=0.5261, Train_RMSE=0.9280, Test_RMSE=0.9891, Train_Recall_at_10=0.0505\n",
      "Epoch 283: Loss=0.5242, Train_RMSE=0.9279, Test_RMSE=0.9890, Train_Recall_at_10=0.0500\n",
      "Epoch 284: Loss=0.5253, Train_RMSE=0.9277, Test_RMSE=0.9889, Train_Recall_at_10=0.0497\n",
      "Epoch 285: Loss=0.5259, Train_RMSE=0.9276, Test_RMSE=0.9889, Train_Recall_at_10=0.0496\n",
      "Epoch 286: Loss=0.5266, Train_RMSE=0.9275, Test_RMSE=0.9888, Train_Recall_at_10=0.0496\n",
      "Epoch 287: Loss=0.5269, Train_RMSE=0.9274, Test_RMSE=0.9887, Train_Recall_at_10=0.0496\n",
      "Epoch 288: Loss=0.5256, Train_RMSE=0.9273, Test_RMSE=0.9886, Train_Recall_at_10=0.0496\n",
      "Epoch 289: Loss=0.5236, Train_RMSE=0.9272, Test_RMSE=0.9885, Train_Recall_at_10=0.0496\n",
      "Epoch 290: Loss=0.5255, Train_RMSE=0.9271, Test_RMSE=0.9884, Train_Recall_at_10=0.0496\n",
      "Epoch 291: Loss=0.5263, Train_RMSE=0.9270, Test_RMSE=0.9883, Train_Recall_at_10=0.0496\n",
      "Epoch 292: Loss=0.5266, Train_RMSE=0.9269, Test_RMSE=0.9882, Train_Recall_at_10=0.0496\n",
      "Epoch 293: Loss=0.5238, Train_RMSE=0.9268, Test_RMSE=0.9881, Train_Recall_at_10=0.0496\n",
      "Epoch 294: Loss=0.5254, Train_RMSE=0.9267, Test_RMSE=0.9880, Train_Recall_at_10=0.0496\n",
      "Epoch 295: Loss=0.5263, Train_RMSE=0.9266, Test_RMSE=0.9879, Train_Recall_at_10=0.0496\n",
      "Epoch 296: Loss=0.5247, Train_RMSE=0.9265, Test_RMSE=0.9878, Train_Recall_at_10=0.0496\n",
      "Epoch 297: Loss=0.5274, Train_RMSE=0.9263, Test_RMSE=0.9877, Train_Recall_at_10=0.0496\n",
      "Epoch 298: Loss=0.5239, Train_RMSE=0.9262, Test_RMSE=0.9876, Train_Recall_at_10=0.0496\n",
      "Epoch 299: Loss=0.5253, Train_RMSE=0.9261, Test_RMSE=0.9875, Train_Recall_at_10=0.0496\n",
      "Epoch 300: Loss=0.5257, Train_RMSE=0.9260, Test_RMSE=0.9874, Train_Recall_at_10=0.0496\n",
      "Epoch 301: Loss=0.5254, Train_RMSE=0.9259, Test_RMSE=0.9873, Train_Recall_at_10=0.0496\n",
      "Epoch 302: Loss=0.5233, Train_RMSE=0.9259, Test_RMSE=0.9872, Train_Recall_at_10=0.0496\n",
      "Epoch 303: Loss=0.5255, Train_RMSE=0.9258, Test_RMSE=0.9871, Train_Recall_at_10=0.0496\n",
      "Epoch 304: Loss=0.5268, Train_RMSE=0.9257, Test_RMSE=0.9870, Train_Recall_at_10=0.0496\n",
      "Epoch 305: Loss=0.5266, Train_RMSE=0.9256, Test_RMSE=0.9870, Train_Recall_at_10=0.0496\n",
      "Epoch 306: Loss=0.5252, Train_RMSE=0.9255, Test_RMSE=0.9869, Train_Recall_at_10=0.0496\n",
      "Epoch 307: Loss=0.5260, Train_RMSE=0.9255, Test_RMSE=0.9868, Train_Recall_at_10=0.0494\n",
      "Epoch 308: Loss=0.5253, Train_RMSE=0.9254, Test_RMSE=0.9867, Train_Recall_at_10=0.0488\n",
      "Epoch 309: Loss=0.5257, Train_RMSE=0.9253, Test_RMSE=0.9866, Train_Recall_at_10=0.0482\n",
      "Epoch 310: Loss=0.5254, Train_RMSE=0.9253, Test_RMSE=0.9865, Train_Recall_at_10=0.0481\n",
      "Epoch 311: Loss=0.5236, Train_RMSE=0.9252, Test_RMSE=0.9865, Train_Recall_at_10=0.0480\n",
      "Epoch 312: Loss=0.5238, Train_RMSE=0.9252, Test_RMSE=0.9864, Train_Recall_at_10=0.0480\n",
      "Epoch 313: Loss=0.5255, Train_RMSE=0.9251, Test_RMSE=0.9863, Train_Recall_at_10=0.0480\n",
      "Epoch 314: Loss=0.5237, Train_RMSE=0.9250, Test_RMSE=0.9863, Train_Recall_at_10=0.0480\n",
      "Epoch 315: Loss=0.5250, Train_RMSE=0.9249, Test_RMSE=0.9862, Train_Recall_at_10=0.0480\n",
      "Epoch 316: Loss=0.5257, Train_RMSE=0.9249, Test_RMSE=0.9861, Train_Recall_at_10=0.0480\n",
      "Epoch 317: Loss=0.5255, Train_RMSE=0.9248, Test_RMSE=0.9861, Train_Recall_at_10=0.0480\n",
      "Epoch 318: Loss=0.5232, Train_RMSE=0.9247, Test_RMSE=0.9860, Train_Recall_at_10=0.0480\n",
      "Epoch 319: Loss=0.5244, Train_RMSE=0.9247, Test_RMSE=0.9860, Train_Recall_at_10=0.0480\n",
      "Epoch 320: Loss=0.5259, Train_RMSE=0.9246, Test_RMSE=0.9859, Train_Recall_at_10=0.0480\n",
      "Epoch 321: Loss=0.5243, Train_RMSE=0.9245, Test_RMSE=0.9859, Train_Recall_at_10=0.0480\n",
      "Epoch 322: Loss=0.5247, Train_RMSE=0.9245, Test_RMSE=0.9858, Train_Recall_at_10=0.0480\n",
      "Epoch 323: Loss=0.5246, Train_RMSE=0.9244, Test_RMSE=0.9858, Train_Recall_at_10=0.0480\n",
      "Epoch 324: Loss=0.5252, Train_RMSE=0.9244, Test_RMSE=0.9857, Train_Recall_at_10=0.0480\n",
      "Epoch 325: Loss=0.5250, Train_RMSE=0.9243, Test_RMSE=0.9857, Train_Recall_at_10=0.0480\n",
      "Epoch 326: Loss=0.5229, Train_RMSE=0.9243, Test_RMSE=0.9856, Train_Recall_at_10=0.0480\n",
      "Epoch 327: Loss=0.5248, Train_RMSE=0.9242, Test_RMSE=0.9856, Train_Recall_at_10=0.0480\n",
      "Epoch 328: Loss=0.5273, Train_RMSE=0.9241, Test_RMSE=0.9856, Train_Recall_at_10=0.0480\n",
      "Epoch 329: Loss=0.5242, Train_RMSE=0.9241, Test_RMSE=0.9855, Train_Recall_at_10=0.0480\n",
      "Epoch 330: Loss=0.5265, Train_RMSE=0.9240, Test_RMSE=0.9855, Train_Recall_at_10=0.0480\n",
      "Epoch 331: Loss=0.5256, Train_RMSE=0.9240, Test_RMSE=0.9854, Train_Recall_at_10=0.0480\n",
      "Epoch 332: Loss=0.5246, Train_RMSE=0.9240, Test_RMSE=0.9854, Train_Recall_at_10=0.0480\n",
      "Epoch 333: Loss=0.5230, Train_RMSE=0.9239, Test_RMSE=0.9853, Train_Recall_at_10=0.0480\n",
      "Epoch 334: Loss=0.5254, Train_RMSE=0.9239, Test_RMSE=0.9853, Train_Recall_at_10=0.0480\n",
      "Epoch 335: Loss=0.5259, Train_RMSE=0.9238, Test_RMSE=0.9852, Train_Recall_at_10=0.0480\n",
      "Epoch 336: Loss=0.5220, Train_RMSE=0.9238, Test_RMSE=0.9852, Train_Recall_at_10=0.0480\n",
      "Epoch 337: Loss=0.5263, Train_RMSE=0.9237, Test_RMSE=0.9852, Train_Recall_at_10=0.0481\n",
      "Epoch 338: Loss=0.5263, Train_RMSE=0.9237, Test_RMSE=0.9851, Train_Recall_at_10=0.0476\n",
      "Epoch 339: Loss=0.5263, Train_RMSE=0.9237, Test_RMSE=0.9851, Train_Recall_at_10=0.0477\n",
      "Epoch 340: Loss=0.5245, Train_RMSE=0.9236, Test_RMSE=0.9851, Train_Recall_at_10=0.0479\n",
      "Epoch 341: Loss=0.5249, Train_RMSE=0.9236, Test_RMSE=0.9850, Train_Recall_at_10=0.0477\n",
      "Epoch 342: Loss=0.5250, Train_RMSE=0.9236, Test_RMSE=0.9850, Train_Recall_at_10=0.0479\n",
      "Epoch 343: Loss=0.5255, Train_RMSE=0.9235, Test_RMSE=0.9850, Train_Recall_at_10=0.0477\n",
      "Epoch 344: Loss=0.5259, Train_RMSE=0.9235, Test_RMSE=0.9849, Train_Recall_at_10=0.0477\n",
      "Epoch 345: Loss=0.5252, Train_RMSE=0.9235, Test_RMSE=0.9849, Train_Recall_at_10=0.0482\n",
      "Epoch 346: Loss=0.5279, Train_RMSE=0.9234, Test_RMSE=0.9849, Train_Recall_at_10=0.0481\n",
      "Epoch 347: Loss=0.5253, Train_RMSE=0.9234, Test_RMSE=0.9848, Train_Recall_at_10=0.0481\n",
      "Epoch 348: Loss=0.5228, Train_RMSE=0.9234, Test_RMSE=0.9848, Train_Recall_at_10=0.0480\n",
      "Epoch 349: Loss=0.5252, Train_RMSE=0.9234, Test_RMSE=0.9848, Train_Recall_at_10=0.0480\n",
      "Epoch 350: Loss=0.5250, Train_RMSE=0.9233, Test_RMSE=0.9847, Train_Recall_at_10=0.0480\n",
      "Epoch 351: Loss=0.5254, Train_RMSE=0.9233, Test_RMSE=0.9847, Train_Recall_at_10=0.0480\n",
      "Epoch 352: Loss=0.5263, Train_RMSE=0.9232, Test_RMSE=0.9846, Train_Recall_at_10=0.0481\n",
      "Epoch 353: Loss=0.5269, Train_RMSE=0.9232, Test_RMSE=0.9846, Train_Recall_at_10=0.0481\n",
      "Epoch 354: Loss=0.5247, Train_RMSE=0.9231, Test_RMSE=0.9845, Train_Recall_at_10=0.0480\n",
      "Epoch 355: Loss=0.5264, Train_RMSE=0.9230, Test_RMSE=0.9844, Train_Recall_at_10=0.0473\n",
      "Epoch 356: Loss=0.5241, Train_RMSE=0.9230, Test_RMSE=0.9844, Train_Recall_at_10=0.0475\n",
      "Epoch 357: Loss=0.5253, Train_RMSE=0.9229, Test_RMSE=0.9843, Train_Recall_at_10=0.0475\n",
      "Epoch 358: Loss=0.5269, Train_RMSE=0.9228, Test_RMSE=0.9842, Train_Recall_at_10=0.0475\n",
      "Epoch 359: Loss=0.5248, Train_RMSE=0.9227, Test_RMSE=0.9842, Train_Recall_at_10=0.0475\n",
      "Epoch 360: Loss=0.5260, Train_RMSE=0.9226, Test_RMSE=0.9841, Train_Recall_at_10=0.0475\n",
      "Epoch 361: Loss=0.5252, Train_RMSE=0.9225, Test_RMSE=0.9840, Train_Recall_at_10=0.0475\n",
      "Epoch 362: Loss=0.5259, Train_RMSE=0.9224, Test_RMSE=0.9839, Train_Recall_at_10=0.0475\n",
      "Epoch 363: Loss=0.5247, Train_RMSE=0.9223, Test_RMSE=0.9839, Train_Recall_at_10=0.0475\n",
      "Epoch 364: Loss=0.5245, Train_RMSE=0.9223, Test_RMSE=0.9838, Train_Recall_at_10=0.0475\n",
      "Epoch 365: Loss=0.5247, Train_RMSE=0.9222, Test_RMSE=0.9837, Train_Recall_at_10=0.0475\n",
      "Epoch 366: Loss=0.5248, Train_RMSE=0.9221, Test_RMSE=0.9836, Train_Recall_at_10=0.0475\n",
      "Epoch 367: Loss=0.5260, Train_RMSE=0.9221, Test_RMSE=0.9836, Train_Recall_at_10=0.0475\n",
      "Epoch 368: Loss=0.5240, Train_RMSE=0.9220, Test_RMSE=0.9835, Train_Recall_at_10=0.0475\n",
      "Epoch 369: Loss=0.5257, Train_RMSE=0.9220, Test_RMSE=0.9835, Train_Recall_at_10=0.0475\n",
      "Epoch 370: Loss=0.5257, Train_RMSE=0.9220, Test_RMSE=0.9834, Train_Recall_at_10=0.0475\n",
      "Epoch 371: Loss=0.5246, Train_RMSE=0.9219, Test_RMSE=0.9834, Train_Recall_at_10=0.0475\n",
      "Epoch 372: Loss=0.5259, Train_RMSE=0.9219, Test_RMSE=0.9833, Train_Recall_at_10=0.0475\n",
      "Epoch 373: Loss=0.5252, Train_RMSE=0.9218, Test_RMSE=0.9833, Train_Recall_at_10=0.0475\n",
      "Epoch 374: Loss=0.5264, Train_RMSE=0.9218, Test_RMSE=0.9832, Train_Recall_at_10=0.0475\n",
      "Epoch 375: Loss=0.5254, Train_RMSE=0.9217, Test_RMSE=0.9831, Train_Recall_at_10=0.0475\n",
      "Epoch 376: Loss=0.5259, Train_RMSE=0.9217, Test_RMSE=0.9831, Train_Recall_at_10=0.0475\n",
      "Epoch 377: Loss=0.5252, Train_RMSE=0.9217, Test_RMSE=0.9830, Train_Recall_at_10=0.0475\n",
      "Epoch 378: Loss=0.5260, Train_RMSE=0.9216, Test_RMSE=0.9829, Train_Recall_at_10=0.0474\n",
      "Epoch 379: Loss=0.5246, Train_RMSE=0.9216, Test_RMSE=0.9829, Train_Recall_at_10=0.0484\n",
      "Epoch 380: Loss=0.5267, Train_RMSE=0.9216, Test_RMSE=0.9828, Train_Recall_at_10=0.0474\n",
      "Epoch 381: Loss=0.5254, Train_RMSE=0.9215, Test_RMSE=0.9828, Train_Recall_at_10=0.0470\n",
      "Epoch 382: Loss=0.5261, Train_RMSE=0.9215, Test_RMSE=0.9827, Train_Recall_at_10=0.0461\n",
      "Epoch 383: Loss=0.5255, Train_RMSE=0.9215, Test_RMSE=0.9827, Train_Recall_at_10=0.0464\n",
      "Epoch 384: Loss=0.5250, Train_RMSE=0.9215, Test_RMSE=0.9826, Train_Recall_at_10=0.0459\n",
      "Epoch 385: Loss=0.5246, Train_RMSE=0.9214, Test_RMSE=0.9826, Train_Recall_at_10=0.0461\n",
      "Epoch 386: Loss=0.5243, Train_RMSE=0.9214, Test_RMSE=0.9825, Train_Recall_at_10=0.0454\n",
      "Epoch 387: Loss=0.5253, Train_RMSE=0.9214, Test_RMSE=0.9825, Train_Recall_at_10=0.0459\n",
      "Epoch 388: Loss=0.5242, Train_RMSE=0.9214, Test_RMSE=0.9824, Train_Recall_at_10=0.0459\n",
      "Epoch 389: Loss=0.5256, Train_RMSE=0.9214, Test_RMSE=0.9824, Train_Recall_at_10=0.0459\n",
      "Epoch 390: Loss=0.5254, Train_RMSE=0.9214, Test_RMSE=0.9824, Train_Recall_at_10=0.0459\n",
      "Epoch 391: Loss=0.5264, Train_RMSE=0.9214, Test_RMSE=0.9823, Train_Recall_at_10=0.0459\n",
      "Epoch 392: Loss=0.5250, Train_RMSE=0.9213, Test_RMSE=0.9823, Train_Recall_at_10=0.0459\n",
      "Epoch 393: Loss=0.5245, Train_RMSE=0.9213, Test_RMSE=0.9822, Train_Recall_at_10=0.0459\n",
      "Epoch 394: Loss=0.5261, Train_RMSE=0.9213, Test_RMSE=0.9822, Train_Recall_at_10=0.0459\n",
      "Epoch 395: Loss=0.5240, Train_RMSE=0.9213, Test_RMSE=0.9822, Train_Recall_at_10=0.0459\n",
      "Epoch 396: Loss=0.5258, Train_RMSE=0.9212, Test_RMSE=0.9821, Train_Recall_at_10=0.0459\n",
      "Epoch 397: Loss=0.5260, Train_RMSE=0.9212, Test_RMSE=0.9821, Train_Recall_at_10=0.0459\n",
      "Epoch 398: Loss=0.5256, Train_RMSE=0.9212, Test_RMSE=0.9820, Train_Recall_at_10=0.0459\n",
      "Epoch 399: Loss=0.5244, Train_RMSE=0.9211, Test_RMSE=0.9820, Train_Recall_at_10=0.0459\n",
      "Epoch 400: Loss=0.5245, Train_RMSE=0.9211, Test_RMSE=0.9819, Train_Recall_at_10=0.0459\n",
      "LightGCN+Attention - Test RMSE: 0.9819, Recall@10: 0.0459\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, edge_index, train_user, train_item, train_rating, \n",
    "                test_user, test_item, test_rating, \n",
    "                num_items, alpha=0.2, epochs=50, lr=0.001, weight_decay=1e-5):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_rmse = float('inf')\n",
    "    best_state = None\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        loss = train_one_epoch(model, optimizer, edge_index, train_user, train_item, train_rating, num_items, alpha)\n",
    "        tr_rmse = evaluate_rmse(model, edge_index, train_user, train_item, train_rating)\n",
    "        val_rmse = evaluate_rmse(model, edge_index, test_user, test_item, test_rating)\n",
    "        train_recall_at_k = recall_at_k(model)\n",
    "        \n",
    "        print(f\"Epoch {epoch:03d}: Loss={loss:.4f}, Train_RMSE={tr_rmse:.4f}, Test_RMSE={val_rmse:.4f}, Train_Recall_at_10={train_recall_at_k:.4f}\")\n",
    "\n",
    "        # Early stopping on validation RMSE\n",
    "        if val_rmse < best_rmse:\n",
    "            best_rmse = val_rmse\n",
    "            best_state = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter > patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Train the baseline model\n",
    "baseline_model = LightGCN(num_nodes=num_nodes, embedding_dim=56, num_layers=3).to(device)\n",
    "print(\"Training Baseline LightGCN...\")\n",
    "baseline_model = train_model(baseline_model, data.edge_index, train_user, train_item, train_rating, \n",
    "                             test_user, test_item, test_rating, num_items, alpha=0.3)\n",
    "\n",
    "baseline_rmse = evaluate_rmse(baseline_model, data.edge_index, test_user, test_item, test_rating)\n",
    "baseline_recall = recall_at_k(baseline_model, k=10)\n",
    "print(f\"Baseline LightGCN - Test RMSE: {baseline_rmse:.4f}, Recall@10: {baseline_recall:.4f}\")\n",
    "\n",
    "# Train the LightGCN with Attention model\n",
    "att_model = LightGCNWithAttention(num_nodes=num_nodes, embedding_dim=56, num_layers=3, dropout=0.4).to(device)\n",
    "print(\"\\nTraining LightGCN with Attention...\")\n",
    "att_model = train_model(att_model, data.edge_index, train_user, train_item, train_rating, \n",
    "                        test_user, test_item, test_rating, num_items, alpha=0.2, \n",
    "                        epochs=400, lr=0.0005, weight_decay=1e-4)\n",
    "\n",
    "att_rmse = evaluate_rmse(att_model, data.edge_index, test_user, test_item, test_rating)\n",
    "att_recall = recall_at_k(att_model, k=10)\n",
    "print(f\"LightGCN+Attention - Test RMSE: {att_rmse:.4f}, Recall@10: {att_recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 12: Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Final Comparison =====\n",
      "Baseline LightGCN    : RMSE = 1.3926, Recall@10 = 0.0839\n",
      "LightGCN + Attention : RMSE = 0.9819, Recall@10 = 0.0459\n"
     ]
    }
   ],
   "source": [
    "print(\"===== Final Comparison =====\")\n",
    "print(f\"Baseline LightGCN    : RMSE = {baseline_rmse:.4f}, Recall@10 = {baseline_recall:.4f}\")\n",
    "print(f\"LightGCN + Attention : RMSE = {att_rmse:.4f}, Recall@10 = {att_recall:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
